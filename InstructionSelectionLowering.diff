//===-- RISCVISelLowering.cpp - RISCV DAG Lowering Implementa	//===-- RISCVISelLowering.cpp - RISCV DAG Lowering Implementa
//								//
// Part of the LLVM Project, under the Apache License v2.0 wi |	//                     The LLVM Compiler Infrastructure
// See https://llvm.org/LICENSE.txt for license information.  |	//
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception    |	// This file is distributed under the University of Illinois 
							      >	// License. See LICENSE.TXT for details.
//								//
//===--------------------------------------------------------	//===--------------------------------------------------------
//								//
// This file defines the interfaces that RISCV uses to lower 	// This file defines the interfaces that RISCV uses to lower 
// selection DAG.						// selection DAG.
//								//
//===--------------------------------------------------------	//===--------------------------------------------------------

#include "RISCVISelLowering.h"					#include "RISCVISelLowering.h"
#include "RISCV.h"						#include "RISCV.h"
#include "RISCVMachineFunctionInfo.h"				#include "RISCVMachineFunctionInfo.h"
#include "RISCVRegisterInfo.h"					#include "RISCVRegisterInfo.h"
#include "RISCVSubtarget.h"					#include "RISCVSubtarget.h"
#include "RISCVTargetMachine.h"					#include "RISCVTargetMachine.h"
#include "llvm/ADT/Statistic.h"					#include "llvm/ADT/Statistic.h"
#include "llvm/CodeGen/CallingConvLower.h"			#include "llvm/CodeGen/CallingConvLower.h"
#include "llvm/CodeGen/MachineFrameInfo.h"			#include "llvm/CodeGen/MachineFrameInfo.h"
#include "llvm/CodeGen/MachineFunction.h"			#include "llvm/CodeGen/MachineFunction.h"
#include "llvm/CodeGen/MachineInstrBuilder.h"			#include "llvm/CodeGen/MachineInstrBuilder.h"
#include "llvm/CodeGen/MachineRegisterInfo.h"			#include "llvm/CodeGen/MachineRegisterInfo.h"
#include "llvm/CodeGen/SelectionDAGISel.h"			#include "llvm/CodeGen/SelectionDAGISel.h"
#include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"		#include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
#include "llvm/CodeGen/ValueTypes.h"				#include "llvm/CodeGen/ValueTypes.h"
#include "llvm/IR/DiagnosticInfo.h"				#include "llvm/IR/DiagnosticInfo.h"
#include "llvm/IR/DiagnosticPrinter.h"				#include "llvm/IR/DiagnosticPrinter.h"
#include "llvm/Support/Debug.h"					#include "llvm/Support/Debug.h"
#include "llvm/Support/ErrorHandling.h"				#include "llvm/Support/ErrorHandling.h"
#include "llvm/Support/raw_ostream.h"				#include "llvm/Support/raw_ostream.h"

using namespace llvm;						using namespace llvm;

#define DEBUG_TYPE "riscv-lower"				#define DEBUG_TYPE "riscv-lower"

STATISTIC(NumTailCalls, "Number of tail calls");		STATISTIC(NumTailCalls, "Number of tail calls");

RISCVTargetLowering::RISCVTargetLowering(const TargetMachine 	RISCVTargetLowering::RISCVTargetLowering(const TargetMachine 
                                         const RISCVSubtarget	                                         const RISCVSubtarget
    : TargetLowering(TM), Subtarget(STI) {			    : TargetLowering(TM), Subtarget(STI) {

  MVT XLenVT = Subtarget.getXLenVT();				  MVT XLenVT = Subtarget.getXLenVT();

  // Set up the register classes.				  // Set up the register classes.
  addRegisterClass(XLenVT, &RISCV::GPRRegClass);		  addRegisterClass(XLenVT, &RISCV::GPRRegClass);

  if (Subtarget.hasStdExtF())					  if (Subtarget.hasStdExtF())
    addRegisterClass(MVT::f32, &RISCV::FPR32RegClass);		    addRegisterClass(MVT::f32, &RISCV::FPR32RegClass);
  if (Subtarget.hasStdExtD())					  if (Subtarget.hasStdExtD())
    addRegisterClass(MVT::f64, &RISCV::FPR64RegClass);		    addRegisterClass(MVT::f64, &RISCV::FPR64RegClass);

  // Compute derived properties from the register classes.	  // Compute derived properties from the register classes.
  computeRegisterProperties(STI.getRegisterInfo());		  computeRegisterProperties(STI.getRegisterInfo());

  setStackPointerRegisterToSaveRestore(RISCV::X2);		  setStackPointerRegisterToSaveRestore(RISCV::X2);

  for (auto N : {ISD::EXTLOAD, ISD::SEXTLOAD, ISD::ZEXTLOAD})	  for (auto N : {ISD::EXTLOAD, ISD::SEXTLOAD, ISD::ZEXTLOAD})
    setLoadExtAction(N, XLenVT, MVT::i1, Promote);		    setLoadExtAction(N, XLenVT, MVT::i1, Promote);

  // TODO: add all necessary setOperationAction calls.		  // TODO: add all necessary setOperationAction calls.
  setOperationAction(ISD::DYNAMIC_STACKALLOC, XLenVT, Expand)	  setOperationAction(ISD::DYNAMIC_STACKALLOC, XLenVT, Expand)

  setOperationAction(ISD::BR_JT, MVT::Other, Expand);		  setOperationAction(ISD::BR_JT, MVT::Other, Expand);
  setOperationAction(ISD::BR_CC, XLenVT, Expand);		  setOperationAction(ISD::BR_CC, XLenVT, Expand);
  setOperationAction(ISD::SELECT, XLenVT, Custom);		  setOperationAction(ISD::SELECT, XLenVT, Custom);
  setOperationAction(ISD::SELECT_CC, XLenVT, Expand);		  setOperationAction(ISD::SELECT_CC, XLenVT, Expand);

  setOperationAction(ISD::STACKSAVE, MVT::Other, Expand);	  setOperationAction(ISD::STACKSAVE, MVT::Other, Expand);
  setOperationAction(ISD::STACKRESTORE, MVT::Other, Expand);	  setOperationAction(ISD::STACKRESTORE, MVT::Other, Expand);

  setOperationAction(ISD::VASTART, MVT::Other, Custom);		  setOperationAction(ISD::VASTART, MVT::Other, Custom);
  setOperationAction(ISD::VAARG, MVT::Other, Expand);		  setOperationAction(ISD::VAARG, MVT::Other, Expand);
  setOperationAction(ISD::VACOPY, MVT::Other, Expand);		  setOperationAction(ISD::VACOPY, MVT::Other, Expand);
  setOperationAction(ISD::VAEND, MVT::Other, Expand);		  setOperationAction(ISD::VAEND, MVT::Other, Expand);

  for (auto VT : {MVT::i1, MVT::i8, MVT::i16})			  for (auto VT : {MVT::i1, MVT::i8, MVT::i16})
    setOperationAction(ISD::SIGN_EXTEND_INREG, VT, Expand);	    setOperationAction(ISD::SIGN_EXTEND_INREG, VT, Expand);

  if (Subtarget.is64Bit()) {					  if (Subtarget.is64Bit()) {
    setTargetDAGCombine(ISD::SHL);				    setTargetDAGCombine(ISD::SHL);
    setTargetDAGCombine(ISD::SRL);				    setTargetDAGCombine(ISD::SRL);
    setTargetDAGCombine(ISD::SRA);				    setTargetDAGCombine(ISD::SRA);
    setTargetDAGCombine(ISD::ANY_EXTEND);			    setTargetDAGCombine(ISD::ANY_EXTEND);
  }								  }

  if (!Subtarget.hasStdExtM()) {				  if (!Subtarget.hasStdExtM()) {
    setOperationAction(ISD::MUL, XLenVT, Expand);		    setOperationAction(ISD::MUL, XLenVT, Expand);
    setOperationAction(ISD::MULHS, XLenVT, Expand);		    setOperationAction(ISD::MULHS, XLenVT, Expand);
    setOperationAction(ISD::MULHU, XLenVT, Expand);		    setOperationAction(ISD::MULHU, XLenVT, Expand);
    setOperationAction(ISD::SDIV, XLenVT, Expand);		    setOperationAction(ISD::SDIV, XLenVT, Expand);
    setOperationAction(ISD::UDIV, XLenVT, Expand);		    setOperationAction(ISD::UDIV, XLenVT, Expand);
    setOperationAction(ISD::SREM, XLenVT, Expand);		    setOperationAction(ISD::SREM, XLenVT, Expand);
    setOperationAction(ISD::UREM, XLenVT, Expand);		    setOperationAction(ISD::UREM, XLenVT, Expand);
  }								  }

  setOperationAction(ISD::SDIVREM, XLenVT, Expand);		  setOperationAction(ISD::SDIVREM, XLenVT, Expand);
  setOperationAction(ISD::UDIVREM, XLenVT, Expand);		  setOperationAction(ISD::UDIVREM, XLenVT, Expand);
  setOperationAction(ISD::SMUL_LOHI, XLenVT, Expand);		  setOperationAction(ISD::SMUL_LOHI, XLenVT, Expand);
  setOperationAction(ISD::UMUL_LOHI, XLenVT, Expand);		  setOperationAction(ISD::UMUL_LOHI, XLenVT, Expand);

  setOperationAction(ISD::SHL_PARTS, XLenVT, Expand);		  setOperationAction(ISD::SHL_PARTS, XLenVT, Expand);
  setOperationAction(ISD::SRL_PARTS, XLenVT, Expand);		  setOperationAction(ISD::SRL_PARTS, XLenVT, Expand);
  setOperationAction(ISD::SRA_PARTS, XLenVT, Expand);		  setOperationAction(ISD::SRA_PARTS, XLenVT, Expand);

  setOperationAction(ISD::ROTL, XLenVT, Expand);		  setOperationAction(ISD::ROTL, XLenVT, Expand);
  setOperationAction(ISD::ROTR, XLenVT, Expand);		  setOperationAction(ISD::ROTR, XLenVT, Expand);
  setOperationAction(ISD::BSWAP, XLenVT, Expand);		  setOperationAction(ISD::BSWAP, XLenVT, Expand);
  setOperationAction(ISD::CTTZ, XLenVT, Expand);		  setOperationAction(ISD::CTTZ, XLenVT, Expand);
  setOperationAction(ISD::CTLZ, XLenVT, Expand);		  setOperationAction(ISD::CTLZ, XLenVT, Expand);
  setOperationAction(ISD::CTPOP, XLenVT, Expand);		  setOperationAction(ISD::CTPOP, XLenVT, Expand);

  ISD::CondCode FPCCToExtend[] = {				  ISD::CondCode FPCCToExtend[] = {
      ISD::SETOGT, ISD::SETOGE, ISD::SETONE, ISD::SETO,   ISD	      ISD::SETOGT, ISD::SETOGE, ISD::SETONE, ISD::SETO,   ISD
      ISD::SETUGT, ISD::SETUGE, ISD::SETULT, ISD::SETULE, ISD	      ISD::SETUGT, ISD::SETUGE, ISD::SETULT, ISD::SETULE, ISD
      ISD::SETGT,  ISD::SETGE,  ISD::SETNE};			      ISD::SETGT,  ISD::SETGE,  ISD::SETNE};

  ISD::NodeType FPOpToExtend[] = {				  ISD::NodeType FPOpToExtend[] = {
      ISD::FSIN, ISD::FCOS, ISD::FSINCOS, ISD::FPOW, ISD::FRE	      ISD::FSIN, ISD::FCOS, ISD::FSINCOS, ISD::FPOW, ISD::FRE

  if (Subtarget.hasStdExtF()) {					  if (Subtarget.hasStdExtF()) {
    setOperationAction(ISD::FMINNUM, MVT::f32, Legal);		    setOperationAction(ISD::FMINNUM, MVT::f32, Legal);
    setOperationAction(ISD::FMAXNUM, MVT::f32, Legal);		    setOperationAction(ISD::FMAXNUM, MVT::f32, Legal);
    for (auto CC : FPCCToExtend)				    for (auto CC : FPCCToExtend)
      setCondCodeAction(CC, MVT::f32, Expand);			      setCondCodeAction(CC, MVT::f32, Expand);
    setOperationAction(ISD::SELECT_CC, MVT::f32, Expand);	    setOperationAction(ISD::SELECT_CC, MVT::f32, Expand);
    setOperationAction(ISD::SELECT, MVT::f32, Custom);		    setOperationAction(ISD::SELECT, MVT::f32, Custom);
    setOperationAction(ISD::BR_CC, MVT::f32, Expand);		    setOperationAction(ISD::BR_CC, MVT::f32, Expand);
    for (auto Op : FPOpToExtend)				    for (auto Op : FPOpToExtend)
      setOperationAction(Op, MVT::f32, Expand);			      setOperationAction(Op, MVT::f32, Expand);
  }								  }

  if (Subtarget.hasStdExtD()) {					  if (Subtarget.hasStdExtD()) {
    setOperationAction(ISD::FMINNUM, MVT::f64, Legal);		    setOperationAction(ISD::FMINNUM, MVT::f64, Legal);
    setOperationAction(ISD::FMAXNUM, MVT::f64, Legal);		    setOperationAction(ISD::FMAXNUM, MVT::f64, Legal);
    for (auto CC : FPCCToExtend)				    for (auto CC : FPCCToExtend)
      setCondCodeAction(CC, MVT::f64, Expand);			      setCondCodeAction(CC, MVT::f64, Expand);
    setOperationAction(ISD::SELECT_CC, MVT::f64, Expand);	    setOperationAction(ISD::SELECT_CC, MVT::f64, Expand);
    setOperationAction(ISD::SELECT, MVT::f64, Custom);		    setOperationAction(ISD::SELECT, MVT::f64, Custom);
    setOperationAction(ISD::BR_CC, MVT::f64, Expand);		    setOperationAction(ISD::BR_CC, MVT::f64, Expand);
    setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f32, Expand	    setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f32, Expand
    setTruncStoreAction(MVT::f64, MVT::f32, Expand);		    setTruncStoreAction(MVT::f64, MVT::f32, Expand);
    for (auto Op : FPOpToExtend)				    for (auto Op : FPOpToExtend)
      setOperationAction(Op, MVT::f64, Expand);			      setOperationAction(Op, MVT::f64, Expand);
  }								  }

  setOperationAction(ISD::GlobalAddress, XLenVT, Custom);	  setOperationAction(ISD::GlobalAddress, XLenVT, Custom);
  setOperationAction(ISD::BlockAddress, XLenVT, Custom);	  setOperationAction(ISD::BlockAddress, XLenVT, Custom);
  setOperationAction(ISD::ConstantPool, XLenVT, Custom);	  setOperationAction(ISD::ConstantPool, XLenVT, Custom);

  if (Subtarget.hasStdExtA()) {					  if (Subtarget.hasStdExtA()) {
    setMaxAtomicSizeInBitsSupported(Subtarget.getXLen());	    setMaxAtomicSizeInBitsSupported(Subtarget.getXLen());
    setMinCmpXchgSizeInBits(32);				    setMinCmpXchgSizeInBits(32);
  } else {							  } else {
    setMaxAtomicSizeInBitsSupported(0);				    setMaxAtomicSizeInBitsSupported(0);
  }								  }

  setBooleanContents(ZeroOrOneBooleanContent);			  setBooleanContents(ZeroOrOneBooleanContent);

  // Function alignments (log2).				  // Function alignments (log2).
  unsigned FunctionAlignment = Subtarget.hasStdExtC() ? 1 : 2	  unsigned FunctionAlignment = Subtarget.hasStdExtC() ? 1 : 2
  setMinFunctionAlignment(FunctionAlignment);			  setMinFunctionAlignment(FunctionAlignment);
  setPrefFunctionAlignment(FunctionAlignment);			  setPrefFunctionAlignment(FunctionAlignment);

  // Effectively disable jump table generation.			  // Effectively disable jump table generation.
  setMinimumJumpTableEntries(INT_MAX);				  setMinimumJumpTableEntries(INT_MAX);
}								}

EVT RISCVTargetLowering::getSetCCResultType(const DataLayout 	EVT RISCVTargetLowering::getSetCCResultType(const DataLayout 
                                            EVT VT) const {	                                            EVT VT) const {
  if (!VT.isVector())						  if (!VT.isVector())
    return getPointerTy(DL);					    return getPointerTy(DL);
  return VT.changeVectorElementTypeToInteger();			  return VT.changeVectorElementTypeToInteger();
}								}

bool RISCVTargetLowering::getTgtMemIntrinsic(IntrinsicInfo &I	bool RISCVTargetLowering::getTgtMemIntrinsic(IntrinsicInfo &I
                                             const CallInst &	                                             const CallInst &
                                             MachineFunction 	                                             MachineFunction 
                                             unsigned Intrins	                                             unsigned Intrins
  switch (Intrinsic) {						  switch (Intrinsic) {
  default:							  default:
    return false;						    return false;
  case Intrinsic::riscv_masked_atomicrmw_xchg_i32:		  case Intrinsic::riscv_masked_atomicrmw_xchg_i32:
  case Intrinsic::riscv_masked_atomicrmw_add_i32:		  case Intrinsic::riscv_masked_atomicrmw_add_i32:
  case Intrinsic::riscv_masked_atomicrmw_sub_i32:		  case Intrinsic::riscv_masked_atomicrmw_sub_i32:
  case Intrinsic::riscv_masked_atomicrmw_nand_i32:		  case Intrinsic::riscv_masked_atomicrmw_nand_i32:
  case Intrinsic::riscv_masked_atomicrmw_max_i32:		  case Intrinsic::riscv_masked_atomicrmw_max_i32:
  case Intrinsic::riscv_masked_atomicrmw_min_i32:		  case Intrinsic::riscv_masked_atomicrmw_min_i32:
  case Intrinsic::riscv_masked_atomicrmw_umax_i32:		  case Intrinsic::riscv_masked_atomicrmw_umax_i32:
  case Intrinsic::riscv_masked_atomicrmw_umin_i32:		  case Intrinsic::riscv_masked_atomicrmw_umin_i32:
  case Intrinsic::riscv_masked_cmpxchg_i32:			  case Intrinsic::riscv_masked_cmpxchg_i32:
    PointerType *PtrTy = cast<PointerType>(I.getArgOperand(0)	    PointerType *PtrTy = cast<PointerType>(I.getArgOperand(0)
    Info.opc = ISD::INTRINSIC_W_CHAIN;				    Info.opc = ISD::INTRINSIC_W_CHAIN;
    Info.memVT = MVT::getVT(PtrTy->getElementType());		    Info.memVT = MVT::getVT(PtrTy->getElementType());
    Info.ptrVal = I.getArgOperand(0);				    Info.ptrVal = I.getArgOperand(0);
    Info.offset = 0;						    Info.offset = 0;
    Info.align = 4;						    Info.align = 4;
    Info.flags = MachineMemOperand::MOLoad | MachineMemOperan	    Info.flags = MachineMemOperand::MOLoad | MachineMemOperan
                 MachineMemOperand::MOVolatile;			                 MachineMemOperand::MOVolatile;
    return true;						    return true;
  }								  }
}								}

bool RISCVTargetLowering::isLegalAddressingMode(const DataLay	bool RISCVTargetLowering::isLegalAddressingMode(const DataLay
                                                const AddrMod	                                                const AddrMod
                                                unsigned AS,	                                                unsigned AS,
                                                Instruction *	                                                Instruction *
  // No global is ever allowed as a base.			  // No global is ever allowed as a base.
  if (AM.BaseGV)						  if (AM.BaseGV)
    return false;						    return false;

  // Require a 12-bit signed offset.				  // Require a 12-bit signed offset.
  if (!isInt<12>(AM.BaseOffs))					  if (!isInt<12>(AM.BaseOffs))
    return false;						    return false;

  switch (AM.Scale) {						  switch (AM.Scale) {
  case 0: // "r+i" or just "i", depending on HasBaseReg.	  case 0: // "r+i" or just "i", depending on HasBaseReg.
    break;							    break;
  case 1:							  case 1:
    if (!AM.HasBaseReg) // allow "r+i".				    if (!AM.HasBaseReg) // allow "r+i".
      break;							      break;
    return false; // disallow "r+r" or "r+r+i".			    return false; // disallow "r+r" or "r+r+i".
  default:							  default:
    return false;						    return false;
  }								  }

  return true;							  return true;
}								}

bool RISCVTargetLowering::isLegalICmpImmediate(int64_t Imm) c	bool RISCVTargetLowering::isLegalICmpImmediate(int64_t Imm) c
  return isInt<12>(Imm);					  return isInt<12>(Imm);
}								}

bool RISCVTargetLowering::isLegalAddImmediate(int64_t Imm) co	bool RISCVTargetLowering::isLegalAddImmediate(int64_t Imm) co
  return isInt<12>(Imm);					  return isInt<12>(Imm);
}								}

// On RV32, 64-bit integers are split into their high and low	// On RV32, 64-bit integers are split into their high and low
// in two different registers, so the trunc is free since the	// in two different registers, so the trunc is free since the
// just be used.						// just be used.
bool RISCVTargetLowering::isTruncateFree(Type *SrcTy, Type *D	bool RISCVTargetLowering::isTruncateFree(Type *SrcTy, Type *D
  if (Subtarget.is64Bit() || !SrcTy->isIntegerTy() || !DstTy-	  if (Subtarget.is64Bit() || !SrcTy->isIntegerTy() || !DstTy-
    return false;						    return false;
  unsigned SrcBits = SrcTy->getPrimitiveSizeInBits();		  unsigned SrcBits = SrcTy->getPrimitiveSizeInBits();
  unsigned DestBits = DstTy->getPrimitiveSizeInBits();		  unsigned DestBits = DstTy->getPrimitiveSizeInBits();
  return (SrcBits == 64 && DestBits == 32);			  return (SrcBits == 64 && DestBits == 32);
}								}

bool RISCVTargetLowering::isTruncateFree(EVT SrcVT, EVT DstVT	bool RISCVTargetLowering::isTruncateFree(EVT SrcVT, EVT DstVT
  if (Subtarget.is64Bit() || SrcVT.isVector() || DstVT.isVect	  if (Subtarget.is64Bit() || SrcVT.isVector() || DstVT.isVect
      !SrcVT.isInteger() || !DstVT.isInteger())			      !SrcVT.isInteger() || !DstVT.isInteger())
    return false;						    return false;
  unsigned SrcBits = SrcVT.getSizeInBits();			  unsigned SrcBits = SrcVT.getSizeInBits();
  unsigned DestBits = DstVT.getSizeInBits();			  unsigned DestBits = DstVT.getSizeInBits();
  return (SrcBits == 64 && DestBits == 32);			  return (SrcBits == 64 && DestBits == 32);
}								}

bool RISCVTargetLowering::isZExtFree(SDValue Val, EVT VT2) co	bool RISCVTargetLowering::isZExtFree(SDValue Val, EVT VT2) co
  // Zexts are free if they can be combined with a load.	  // Zexts are free if they can be combined with a load.
  if (auto *LD = dyn_cast<LoadSDNode>(Val)) {			  if (auto *LD = dyn_cast<LoadSDNode>(Val)) {
    EVT MemVT = LD->getMemoryVT();				    EVT MemVT = LD->getMemoryVT();
    if ((MemVT == MVT::i8 || MemVT == MVT::i16 ||		    if ((MemVT == MVT::i8 || MemVT == MVT::i16 ||
         (Subtarget.is64Bit() && MemVT == MVT::i32)) &&		         (Subtarget.is64Bit() && MemVT == MVT::i32)) &&
        (LD->getExtensionType() == ISD::NON_EXTLOAD ||		        (LD->getExtensionType() == ISD::NON_EXTLOAD ||
         LD->getExtensionType() == ISD::ZEXTLOAD))		         LD->getExtensionType() == ISD::ZEXTLOAD))
      return true;						      return true;
  }								  }

  return TargetLowering::isZExtFree(Val, VT2);			  return TargetLowering::isZExtFree(Val, VT2);
}								}

bool RISCVTargetLowering::isSExtCheaperThanZExt(EVT SrcVT, EV	bool RISCVTargetLowering::isSExtCheaperThanZExt(EVT SrcVT, EV
  return Subtarget.is64Bit() && SrcVT == MVT::i32 && DstVT ==	  return Subtarget.is64Bit() && SrcVT == MVT::i32 && DstVT ==
}								}

// Changes the condition code and swaps operands if necessary	// Changes the condition code and swaps operands if necessary
// operation matches one of the comparisons supported directl	// operation matches one of the comparisons supported directl
// ISA.								// ISA.
static void normaliseSetCC(SDValue &LHS, SDValue &RHS, ISD::C	static void normaliseSetCC(SDValue &LHS, SDValue &RHS, ISD::C
  switch (CC) {							  switch (CC) {
  default:							  default:
    break;							    break;
  case ISD::SETGT:						  case ISD::SETGT:
  case ISD::SETLE:						  case ISD::SETLE:
  case ISD::SETUGT:						  case ISD::SETUGT:
  case ISD::SETULE:						  case ISD::SETULE:
    CC = ISD::getSetCCSwappedOperands(CC);			    CC = ISD::getSetCCSwappedOperands(CC);
    std::swap(LHS, RHS);					    std::swap(LHS, RHS);
    break;							    break;
  }								  }
}								}

// Return the RISC-V branch opcode that matches the given DAG	// Return the RISC-V branch opcode that matches the given DAG
// condition code. The CondCode must be one of those supporte	// condition code. The CondCode must be one of those supporte
// ISA (see normaliseSetCC).					// ISA (see normaliseSetCC).
static unsigned getBranchOpcodeForIntCondCode(ISD::CondCode C	static unsigned getBranchOpcodeForIntCondCode(ISD::CondCode C
  switch (CC) {							  switch (CC) {
  default:							  default:
    llvm_unreachable("Unsupported CondCode");			    llvm_unreachable("Unsupported CondCode");
  case ISD::SETEQ:						  case ISD::SETEQ:
    return RISCV::BEQ;						    return RISCV::BEQ;
  case ISD::SETNE:						  case ISD::SETNE:
    return RISCV::BNE;						    return RISCV::BNE;
  case ISD::SETLT:						  case ISD::SETLT:
    return RISCV::BLT;						    return RISCV::BLT;
  case ISD::SETGE:						  case ISD::SETGE:
    return RISCV::BGE;						    return RISCV::BGE;
  case ISD::SETULT:						  case ISD::SETULT:
    return RISCV::BLTU;						    return RISCV::BLTU;
  case ISD::SETUGE:						  case ISD::SETUGE:
    return RISCV::BGEU;						    return RISCV::BGEU;
  }								  }
}								}

SDValue RISCVTargetLowering::LowerOperation(SDValue Op,		SDValue RISCVTargetLowering::LowerOperation(SDValue Op,
                                            SelectionDAG &DAG	                                            SelectionDAG &DAG
  switch (Op.getOpcode()) {					  switch (Op.getOpcode()) {
  default:							  default:
    report_fatal_error("unimplemented operand");		    report_fatal_error("unimplemented operand");
  case ISD::GlobalAddress:					  case ISD::GlobalAddress:
    return lowerGlobalAddress(Op, DAG);				    return lowerGlobalAddress(Op, DAG);
  case ISD::BlockAddress:					  case ISD::BlockAddress:
    return lowerBlockAddress(Op, DAG);				    return lowerBlockAddress(Op, DAG);
  case ISD::ConstantPool:					  case ISD::ConstantPool:
    return lowerConstantPool(Op, DAG);				    return lowerConstantPool(Op, DAG);
  case ISD::SELECT:						  case ISD::SELECT:
    return lowerSELECT(Op, DAG);				    return lowerSELECT(Op, DAG);
  case ISD::VASTART:						  case ISD::VASTART:
    return lowerVASTART(Op, DAG);				    return lowerVASTART(Op, DAG);
  case ISD::FRAMEADDR:						  case ISD::FRAMEADDR:
    return lowerFRAMEADDR(Op, DAG);				    return lowerFRAMEADDR(Op, DAG);
  case ISD::RETURNADDR:						  case ISD::RETURNADDR:
    return lowerRETURNADDR(Op, DAG);				    return lowerRETURNADDR(Op, DAG);
  }								  }
}								}

SDValue RISCVTargetLowering::lowerGlobalAddress(SDValue Op,	SDValue RISCVTargetLowering::lowerGlobalAddress(SDValue Op,
                                                SelectionDAG 	                                                SelectionDAG 
							      >	  EVT Ty = Op.getValueType();
							      >	  if(Ty == MVT::i32){
							      >		return getGlobalAddressNode(Op);
							      >	  }
							      >	  highLowAddressPair = splitAddress(Op, DAG);
							      >	  for(SDValue halfAddressOp : highLowAddressPair){
							      >	    addressNode.push_back(getGlobalAddressNode(halfAddressOp)
							      >	  }
							      >	  return SDValue(DAG.getMachineNode(RISCV::LDW, Ty, addressNo
							      >	}
							      >
							      >	SDValue RISCVTargetLowering::getGlobalAddressNode(SDValue Op,
							      >	                                                SelectionDAG 
  SDLoc DL(Op);							  SDLoc DL(Op);
  EVT Ty = Op.getValueType();					  EVT Ty = Op.getValueType();
  GlobalAddressSDNode *N = cast<GlobalAddressSDNode>(Op);	  GlobalAddressSDNode *N = cast<GlobalAddressSDNode>(Op);
  const GlobalValue *GV = N->getGlobal();			  const GlobalValue *GV = N->getGlobal();
  int64_t Offset = N->getOffset();				  int64_t Offset = N->getOffset();
  MVT XLenVT = Subtarget.getXLenVT();				  MVT XLenVT = Subtarget.getXLenVT();

  if (isPositionIndependent())					  if (isPositionIndependent())
    report_fatal_error("Unable to lowerGlobalAddress");		    report_fatal_error("Unable to lowerGlobalAddress");
  // In order to maximise the opportunity for common subexpre	  // In order to maximise the opportunity for common subexpre
  // emit a separate ADD node for the global address offset i	  // emit a separate ADD node for the global address offset i
  // it in the global address node. Later peephole optimisati	  // it in the global address node. Later peephole optimisati
  // fold it back in when profitable.				  // fold it back in when profitable.
  SDValue GAHi = DAG.getTargetGlobalAddress(GV, DL, Ty, 0, RI	  SDValue GAHi = DAG.getTargetGlobalAddress(GV, DL, Ty, 0, RI
  SDValue GALo = DAG.getTargetGlobalAddress(GV, DL, Ty, 0, RI	  SDValue GALo = DAG.getTargetGlobalAddress(GV, DL, Ty, 0, RI
  SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL, T	  SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL, T
  SDValue MNLo =						  SDValue MNLo =
    SDValue(DAG.getMachineNode(RISCV::ADDI, DL, Ty, MNHi, GAL	    SDValue(DAG.getMachineNode(RISCV::ADDI, DL, Ty, MNHi, GAL
  if (Offset != 0)						  if (Offset != 0)
    return DAG.getNode(ISD::ADD, DL, Ty, MNLo,			    return DAG.getNode(ISD::ADD, DL, Ty, MNLo,
                       DAG.getConstant(Offset, DL, XLenVT));	                       DAG.getConstant(Offset, DL, XLenVT));
  return MNLo;							  return MNLo;
}								}

							      >
SDValue RISCVTargetLowering::lowerBlockAddress(SDValue Op,	SDValue RISCVTargetLowering::lowerBlockAddress(SDValue Op,
                                               SelectionDAG &	                                               SelectionDAG &
							      >	  std::vector<SDValue> highLowAddressPair, addressNode;
							      >	  EVT Ty = Op.getValueType();
							      >	  if(Ty == MVT::i32){
							      >		return getBlockAddressNode(Op);
							      >	  }
							      >	  highLowAddressPair = splitAddress(Op, DAG);
							      >	  for(SDValue halfAddressOp : highLowAddressPair){
							      >	    addressNode.push_back(getBlockAddressNode(halfAddressOp))
							      >	  }
							      >	  return SDValue(DAG.getMachineNode(RISCV::LDW, Ty, addressNo
							      >	}
							      >
							      >	SDValue RISCVTargetLowering::getBlockAddressNode(SDValue Op, 
  SDLoc DL(Op);							  SDLoc DL(Op);
  EVT Ty = Op.getValueType();					  EVT Ty = Op.getValueType();
  BlockAddressSDNode *N = cast<BlockAddressSDNode>(Op);		  BlockAddressSDNode *N = cast<BlockAddressSDNode>(Op);
  const BlockAddress *BA = N->getBlockAddress();		  const BlockAddress *BA = N->getBlockAddress();
  int64_t Offset = N->getOffset();				  int64_t Offset = N->getOffset();

  if (isPositionIndependent())					  if (isPositionIndependent())
    report_fatal_error("Unable to lowerBlockAddress");		    report_fatal_error("Unable to lowerBlockAddress");

  SDValue BAHi = DAG.getTargetBlockAddress(BA, Ty, Offset, RI	  SDValue BAHi = DAG.getTargetBlockAddress(BA, Ty, Offset, RI
  SDValue BALo = DAG.getTargetBlockAddress(BA, Ty, Offset, RI	  SDValue BALo = DAG.getTargetBlockAddress(BA, Ty, Offset, RI
  SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL, T	  SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL, T
  SDValue MNLo =						  SDValue MNLo =
    SDValue(DAG.getMachineNode(RISCV::ADDI, DL, Ty, MNHi, BAL	    SDValue(DAG.getMachineNode(RISCV::ADDI, DL, Ty, MNHi, BAL
  return MNLo;							  return MNLo;
}								}

							      >	std::vector<SDValue> RISCVTargetLowering::splitAddress(SDValu
							      >	  SDLoc DL(Op);
							      >	  SDValue Vec = DAG.getNode(ISD::BITCAST, SL, MVT::i32, Op);
							      >	  const SDValue Zero = DAG.getConstant(0, SL, MVT::i32);
							      >	  const SDValue One = DAG.getConstant(1, SL, MVT::i32);
							      >	  SDValue Lo = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, SL, MVT::
							      >	  SDValue Hi = DAG.getNode(ISD::EXTRACT_VECTOR_ELT, SL, MVT::
							      >	  std::vector<SDValue> highLowAddressPair{Lo, Hi};
							      >	  return highLowAddressPair;
							      >	}
							      >
SDValue RISCVTargetLowering::lowerConstantPool(SDValue Op,	SDValue RISCVTargetLowering::lowerConstantPool(SDValue Op,
                                               SelectionDAG &	                                               SelectionDAG &
  SDLoc DL(Op);							  SDLoc DL(Op);
  EVT Ty = Op.getValueType();					  EVT Ty = Op.getValueType();
  ConstantPoolSDNode *N = cast<ConstantPoolSDNode>(Op);		  ConstantPoolSDNode *N = cast<ConstantPoolSDNode>(Op);
  const Constant *CPA = N->getConstVal();			  const Constant *CPA = N->getConstVal();
  int64_t Offset = N->getOffset();				  int64_t Offset = N->getOffset();
  unsigned Alignment = N->getAlignment();			  unsigned Alignment = N->getAlignment();

  if (!isPositionIndependent()) {				  if (!isPositionIndependent()) {
    SDValue CPAHi =						    SDValue CPAHi =
        DAG.getTargetConstantPool(CPA, Ty, Alignment, Offset,	        DAG.getTargetConstantPool(CPA, Ty, Alignment, Offset,
    SDValue CPALo =						    SDValue CPALo =
        DAG.getTargetConstantPool(CPA, Ty, Alignment, Offset,	        DAG.getTargetConstantPool(CPA, Ty, Alignment, Offset,
    SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL,	    SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL,
    SDValue MNLo =						    SDValue MNLo =
        SDValue(DAG.getMachineNode(RISCV::ADDI, DL, Ty, MNHi,	        SDValue(DAG.getMachineNode(RISCV::ADDI, DL, Ty, MNHi,
    return MNLo;						    return MNLo;
  } else {							  } else {
    report_fatal_error("Unable to lowerConstantPool");		    report_fatal_error("Unable to lowerConstantPool");
  }								  }
}								}

SDValue RISCVTargetLowering::lowerSELECT(SDValue Op, Selectio	SDValue RISCVTargetLowering::lowerSELECT(SDValue Op, Selectio
  SDValue CondV = Op.getOperand(0);				  SDValue CondV = Op.getOperand(0);
  SDValue TrueV = Op.getOperand(1);				  SDValue TrueV = Op.getOperand(1);
  SDValue FalseV = Op.getOperand(2);				  SDValue FalseV = Op.getOperand(2);
  SDLoc DL(Op);							  SDLoc DL(Op);
  MVT XLenVT = Subtarget.getXLenVT();				  MVT XLenVT = Subtarget.getXLenVT();

  // If the result type is XLenVT and CondV is the output of 	  // If the result type is XLenVT and CondV is the output of 
  // which also operated on XLenVT inputs, then merge the SET	  // which also operated on XLenVT inputs, then merge the SET
  // lowered RISCVISD::SELECT_CC to take advantage of the int	  // lowered RISCVISD::SELECT_CC to take advantage of the int
  // compare+branch instructions. i.e.:				  // compare+branch instructions. i.e.:
  // (select (setcc lhs, rhs, cc), truev, falsev)		  // (select (setcc lhs, rhs, cc), truev, falsev)
  // -> (riscvisd::select_cc lhs, rhs, cc, truev, falsev)	  // -> (riscvisd::select_cc lhs, rhs, cc, truev, falsev)
  if (Op.getSimpleValueType() == XLenVT && CondV.getOpcode() 	  if (Op.getSimpleValueType() == XLenVT && CondV.getOpcode() 
      CondV.getOperand(0).getSimpleValueType() == XLenVT) {	      CondV.getOperand(0).getSimpleValueType() == XLenVT) {
    SDValue LHS = CondV.getOperand(0);				    SDValue LHS = CondV.getOperand(0);
    SDValue RHS = CondV.getOperand(1);				    SDValue RHS = CondV.getOperand(1);
    auto CC = cast<CondCodeSDNode>(CondV.getOperand(2));	    auto CC = cast<CondCodeSDNode>(CondV.getOperand(2));
    ISD::CondCode CCVal = CC->get();				    ISD::CondCode CCVal = CC->get();

    normaliseSetCC(LHS, RHS, CCVal);				    normaliseSetCC(LHS, RHS, CCVal);

    SDValue TargetCC = DAG.getConstant(CCVal, DL, XLenVT);	    SDValue TargetCC = DAG.getConstant(CCVal, DL, XLenVT);
    SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::Glue	    SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::Glue
    SDValue Ops[] = {LHS, RHS, TargetCC, TrueV, FalseV};	    SDValue Ops[] = {LHS, RHS, TargetCC, TrueV, FalseV};
    return DAG.getNode(RISCVISD::SELECT_CC, DL, VTs, Ops);	    return DAG.getNode(RISCVISD::SELECT_CC, DL, VTs, Ops);
  }								  }

  // Otherwise:							  // Otherwise:
  // (select condv, truev, falsev)				  // (select condv, truev, falsev)
  // -> (riscvisd::select_cc condv, zero, setne, truev, false	  // -> (riscvisd::select_cc condv, zero, setne, truev, false
  SDValue Zero = DAG.getConstant(0, DL, XLenVT);		  SDValue Zero = DAG.getConstant(0, DL, XLenVT);
  SDValue SetNE = DAG.getConstant(ISD::SETNE, DL, XLenVT);	  SDValue SetNE = DAG.getConstant(ISD::SETNE, DL, XLenVT);

  SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::Glue);	  SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::Glue);
  SDValue Ops[] = {CondV, Zero, SetNE, TrueV, FalseV};		  SDValue Ops[] = {CondV, Zero, SetNE, TrueV, FalseV};

  return DAG.getNode(RISCVISD::SELECT_CC, DL, VTs, Ops);	  return DAG.getNode(RISCVISD::SELECT_CC, DL, VTs, Ops);
}								}

SDValue RISCVTargetLowering::lowerVASTART(SDValue Op, Selecti	SDValue RISCVTargetLowering::lowerVASTART(SDValue Op, Selecti
  MachineFunction &MF = DAG.getMachineFunction();		  MachineFunction &MF = DAG.getMachineFunction();
  RISCVMachineFunctionInfo *FuncInfo = MF.getInfo<RISCVMachin	  RISCVMachineFunctionInfo *FuncInfo = MF.getInfo<RISCVMachin
							      <
  SDLoc DL(Op);							  SDLoc DL(Op);
  SDValue FI = DAG.getFrameIndex(FuncInfo->getVarArgsFrameInd	  SDValue FI = DAG.getFrameIndex(FuncInfo->getVarArgsFrameInd
                                 getPointerTy(MF.getDataLayou	                                 getPointerTy(MF.getDataLayou

  // vastart just stores the address of the VarArgsFrameIndex	  // vastart just stores the address of the VarArgsFrameIndex
  // memory location argument.					  // memory location argument.
  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->g	  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->g
  return DAG.getStore(Op.getOperand(0), DL, FI, Op.getOperand	  return DAG.getStore(Op.getOperand(0), DL, FI, Op.getOperand
                      MachinePointerInfo(SV));			                      MachinePointerInfo(SV));
}								}

SDValue RISCVTargetLowering::lowerFRAMEADDR(SDValue Op,		SDValue RISCVTargetLowering::lowerFRAMEADDR(SDValue Op,
                                            SelectionDAG &DAG	                                            SelectionDAG &DAG
  const RISCVRegisterInfo &RI = *Subtarget.getRegisterInfo();	  const RISCVRegisterInfo &RI = *Subtarget.getRegisterInfo();
  MachineFunction &MF = DAG.getMachineFunction();		  MachineFunction &MF = DAG.getMachineFunction();
  MachineFrameInfo &MFI = MF.getFrameInfo();			  MachineFrameInfo &MFI = MF.getFrameInfo();
  MFI.setFrameAddressIsTaken(true);				  MFI.setFrameAddressIsTaken(true);
  unsigned FrameReg = RI.getFrameRegister(MF);			  unsigned FrameReg = RI.getFrameRegister(MF);
  int XLenInBytes = Subtarget.getXLen() / 8;			  int XLenInBytes = Subtarget.getXLen() / 8;

  EVT VT = Op.getValueType();					  EVT VT = Op.getValueType();
  SDLoc DL(Op);							  SDLoc DL(Op);
  SDValue FrameAddr = DAG.getCopyFromReg(DAG.getEntryNode(), 	  SDValue FrameAddr = DAG.getCopyFromReg(DAG.getEntryNode(), 
  unsigned Depth = cast<ConstantSDNode>(Op.getOperand(0))->ge	  unsigned Depth = cast<ConstantSDNode>(Op.getOperand(0))->ge
  while (Depth--) {						  while (Depth--) {
    int Offset = -(XLenInBytes * 2);				    int Offset = -(XLenInBytes * 2);
    SDValue Ptr = DAG.getNode(ISD::ADD, DL, VT, FrameAddr,	    SDValue Ptr = DAG.getNode(ISD::ADD, DL, VT, FrameAddr,
                              DAG.getIntPtrConstant(Offset, D	                              DAG.getIntPtrConstant(Offset, D
    FrameAddr =							    FrameAddr =
        DAG.getLoad(VT, DL, DAG.getEntryNode(), Ptr, MachineP	        DAG.getLoad(VT, DL, DAG.getEntryNode(), Ptr, MachineP
  }								  }
  return FrameAddr;						  return FrameAddr;
}								}

SDValue RISCVTargetLowering::lowerRETURNADDR(SDValue Op,	SDValue RISCVTargetLowering::lowerRETURNADDR(SDValue Op,
                                             SelectionDAG &DA	                                             SelectionDAG &DA
  const RISCVRegisterInfo &RI = *Subtarget.getRegisterInfo();	  const RISCVRegisterInfo &RI = *Subtarget.getRegisterInfo();
  MachineFunction &MF = DAG.getMachineFunction();		  MachineFunction &MF = DAG.getMachineFunction();
  MachineFrameInfo &MFI = MF.getFrameInfo();			  MachineFrameInfo &MFI = MF.getFrameInfo();
  MFI.setReturnAddressIsTaken(true);				  MFI.setReturnAddressIsTaken(true);
  MVT XLenVT = Subtarget.getXLenVT();				  MVT XLenVT = Subtarget.getXLenVT();
  int XLenInBytes = Subtarget.getXLen() / 8;			  int XLenInBytes = Subtarget.getXLen() / 8;

  if (verifyReturnAddressArgumentIsConstant(Op, DAG))		  if (verifyReturnAddressArgumentIsConstant(Op, DAG))
    return SDValue();						    return SDValue();

  EVT VT = Op.getValueType();					  EVT VT = Op.getValueType();
  SDLoc DL(Op);							  SDLoc DL(Op);
  unsigned Depth = cast<ConstantSDNode>(Op.getOperand(0))->ge	  unsigned Depth = cast<ConstantSDNode>(Op.getOperand(0))->ge
  if (Depth) {							  if (Depth) {
    int Off = -XLenInBytes;					    int Off = -XLenInBytes;
    SDValue FrameAddr = lowerFRAMEADDR(Op, DAG);		    SDValue FrameAddr = lowerFRAMEADDR(Op, DAG);
    SDValue Offset = DAG.getConstant(Off, DL, VT);		    SDValue Offset = DAG.getConstant(Off, DL, VT);
    return DAG.getLoad(VT, DL, DAG.getEntryNode(),		    return DAG.getLoad(VT, DL, DAG.getEntryNode(),
                       DAG.getNode(ISD::ADD, DL, VT, FrameAdd	                       DAG.getNode(ISD::ADD, DL, VT, FrameAdd
                       MachinePointerInfo());			                       MachinePointerInfo());
  }								  }

  // Return the value of the return address register, marking	  // Return the value of the return address register, marking
  // live-in.							  // live-in.
  unsigned Reg = MF.addLiveIn(RI.getRARegister(), getRegClass	  unsigned Reg = MF.addLiveIn(RI.getRARegister(), getRegClass
  return DAG.getCopyFromReg(DAG.getEntryNode(), DL, Reg, XLen	  return DAG.getCopyFromReg(DAG.getEntryNode(), DL, Reg, XLen
}								}

// Return true if the given node is a shift with a non-consta	// Return true if the given node is a shift with a non-consta
static bool isVariableShift(SDValue Val) {			static bool isVariableShift(SDValue Val) {
  switch (Val.getOpcode()) {					  switch (Val.getOpcode()) {
  default:							  default:
    return false;						    return false;
  case ISD::SHL:						  case ISD::SHL:
  case ISD::SRA:						  case ISD::SRA:
  case ISD::SRL:						  case ISD::SRL:
    return Val.getOperand(1).getOpcode() != ISD::Constant;	    return Val.getOperand(1).getOpcode() != ISD::Constant;
  }								  }
}								}

// Returns true if the given node is an sdiv, udiv, or urem w	// Returns true if the given node is an sdiv, udiv, or urem w
// operands.							// operands.
static bool isVariableSDivUDivURem(SDValue Val) {		static bool isVariableSDivUDivURem(SDValue Val) {
  switch (Val.getOpcode()) {					  switch (Val.getOpcode()) {
  default:							  default:
    return false;						    return false;
  case ISD::SDIV:						  case ISD::SDIV:
  case ISD::UDIV:						  case ISD::UDIV:
  case ISD::UREM:						  case ISD::UREM:
    return Val.getOperand(0).getOpcode() != ISD::Constant &&	    return Val.getOperand(0).getOpcode() != ISD::Constant &&
           Val.getOperand(1).getOpcode() != ISD::Constant;	           Val.getOperand(1).getOpcode() != ISD::Constant;
  }								  }
}								}

SDValue RISCVTargetLowering::PerformDAGCombine(SDNode *N,	SDValue RISCVTargetLowering::PerformDAGCombine(SDNode *N,
                                               DAGCombinerInf	                                               DAGCombinerInf
  SelectionDAG &DAG = DCI.DAG;					  SelectionDAG &DAG = DCI.DAG;

  switch (N->getOpcode()) {					  switch (N->getOpcode()) {
  default:							  default:
    break;							    break;
  case ISD::SHL:						  case ISD::SHL:
  case ISD::SRL:						  case ISD::SRL:
  case ISD::SRA: {						  case ISD::SRA: {
    assert(Subtarget.getXLen() == 64 && "Combine should be 64	    assert(Subtarget.getXLen() == 64 && "Combine should be 64
    if (!DCI.isBeforeLegalize())				    if (!DCI.isBeforeLegalize())
      break;							      break;
    SDValue RHS = N->getOperand(1);				    SDValue RHS = N->getOperand(1);
    if (N->getValueType(0) != MVT::i32 || RHS->getOpcode() ==	    if (N->getValueType(0) != MVT::i32 || RHS->getOpcode() ==
        (RHS->getOpcode() == ISD::AssertZext &&			        (RHS->getOpcode() == ISD::AssertZext &&
         cast<VTSDNode>(RHS->getOperand(1))->getVT().getSizeI	         cast<VTSDNode>(RHS->getOperand(1))->getVT().getSizeI
      break;							      break;
    SDValue LHS = N->getOperand(0);				    SDValue LHS = N->getOperand(0);
    SDLoc DL(N);						    SDLoc DL(N);
    SDValue NewRHS =						    SDValue NewRHS =
        DAG.getNode(ISD::AssertZext, DL, RHS.getValueType(), 	        DAG.getNode(ISD::AssertZext, DL, RHS.getValueType(), 
                    DAG.getValueType(EVT::getIntegerVT(*DAG.g	                    DAG.getValueType(EVT::getIntegerVT(*DAG.g
    return DCI.CombineTo(					    return DCI.CombineTo(
        N, DAG.getNode(N->getOpcode(), DL, LHS.getValueType()	        N, DAG.getNode(N->getOpcode(), DL, LHS.getValueType()
  }								  }
  case ISD::ANY_EXTEND: {					  case ISD::ANY_EXTEND: {
    // If any-extending an i32 variable-length shift or sdiv/	    // If any-extending an i32 variable-length shift or sdiv/
    // then instead sign-extend in order to increase the chan	    // then instead sign-extend in order to increase the chan
    // to select the sllw/srlw/sraw/divw/divuw/remuw instruct	    // to select the sllw/srlw/sraw/divw/divuw/remuw instruct
    SDValue Src = N->getOperand(0);				    SDValue Src = N->getOperand(0);
    if (N->getValueType(0) != MVT::i64 || Src.getValueType() 	    if (N->getValueType(0) != MVT::i64 || Src.getValueType() 
      break;							      break;
    if (!isVariableShift(Src) &&				    if (!isVariableShift(Src) &&
        !(Subtarget.hasStdExtM() && isVariableSDivUDivURem(Sr	        !(Subtarget.hasStdExtM() && isVariableSDivUDivURem(Sr
      break;							      break;
    SDLoc DL(N);						    SDLoc DL(N);
    return DCI.CombineTo(N, DAG.getNode(ISD::SIGN_EXTEND, DL,	    return DCI.CombineTo(N, DAG.getNode(ISD::SIGN_EXTEND, DL,
  }								  }
  case RISCVISD::SplitF64: {					  case RISCVISD::SplitF64: {
    // If the input to SplitF64 is just BuildPairF64 then the	    // If the input to SplitF64 is just BuildPairF64 then the
    // redundant. Instead, use BuildPairF64's operands direct	    // redundant. Instead, use BuildPairF64's operands direct
    SDValue Op0 = N->getOperand(0);				    SDValue Op0 = N->getOperand(0);
    if (Op0->getOpcode() != RISCVISD::BuildPairF64)		    if (Op0->getOpcode() != RISCVISD::BuildPairF64)
      break;							      break;
    return DCI.CombineTo(N, Op0.getOperand(0), Op0.getOperand	    return DCI.CombineTo(N, Op0.getOperand(0), Op0.getOperand
  }								  }
  }								  }

  return SDValue();						  return SDValue();
}								}

static MachineBasicBlock *emitSplitF64Pseudo(MachineInstr &MI	static MachineBasicBlock *emitSplitF64Pseudo(MachineInstr &MI
                                             MachineBasicBloc	                                             MachineBasicBloc
  assert(MI.getOpcode() == RISCV::SplitF64Pseudo && "Unexpect	  assert(MI.getOpcode() == RISCV::SplitF64Pseudo && "Unexpect

  MachineFunction &MF = *BB->getParent();			  MachineFunction &MF = *BB->getParent();
  DebugLoc DL = MI.getDebugLoc();				  DebugLoc DL = MI.getDebugLoc();
  const TargetInstrInfo &TII = *MF.getSubtarget().getInstrInf	  const TargetInstrInfo &TII = *MF.getSubtarget().getInstrInf
  const TargetRegisterInfo *RI = MF.getSubtarget().getRegiste	  const TargetRegisterInfo *RI = MF.getSubtarget().getRegiste
  unsigned LoReg = MI.getOperand(0).getReg();			  unsigned LoReg = MI.getOperand(0).getReg();
  unsigned HiReg = MI.getOperand(1).getReg();			  unsigned HiReg = MI.getOperand(1).getReg();
  unsigned SrcReg = MI.getOperand(2).getReg();			  unsigned SrcReg = MI.getOperand(2).getReg();
  const TargetRegisterClass *SrcRC = &RISCV::FPR64RegClass;	  const TargetRegisterClass *SrcRC = &RISCV::FPR64RegClass;
  int FI = MF.getInfo<RISCVMachineFunctionInfo>()->getMoveF64	  int FI = MF.getInfo<RISCVMachineFunctionInfo>()->getMoveF64

  TII.storeRegToStackSlot(*BB, MI, SrcReg, MI.getOperand(2).i	  TII.storeRegToStackSlot(*BB, MI, SrcReg, MI.getOperand(2).i
                          RI);					                          RI);
  MachineMemOperand *MMO =					  MachineMemOperand *MMO =
      MF.getMachineMemOperand(MachinePointerInfo::getFixedSta	      MF.getMachineMemOperand(MachinePointerInfo::getFixedSta
                              MachineMemOperand::MOLoad, 8, 8	                              MachineMemOperand::MOLoad, 8, 8
  BuildMI(*BB, MI, DL, TII.get(RISCV::LW), LoReg)		  BuildMI(*BB, MI, DL, TII.get(RISCV::LW), LoReg)
      .addFrameIndex(FI)					      .addFrameIndex(FI)
      .addImm(0)						      .addImm(0)
      .addMemOperand(MMO);					      .addMemOperand(MMO);
  BuildMI(*BB, MI, DL, TII.get(RISCV::LW), HiReg)		  BuildMI(*BB, MI, DL, TII.get(RISCV::LW), HiReg)
      .addFrameIndex(FI)					      .addFrameIndex(FI)
      .addImm(4)						      .addImm(4)
      .addMemOperand(MMO);					      .addMemOperand(MMO);
  MI.eraseFromParent(); // The pseudo instruction is gone now	  MI.eraseFromParent(); // The pseudo instruction is gone now
  return BB;							  return BB;
}								}

static MachineBasicBlock *emitBuildPairF64Pseudo(MachineInstr	static MachineBasicBlock *emitBuildPairF64Pseudo(MachineInstr
                                                 MachineBasic	                                                 MachineBasic
  assert(MI.getOpcode() == RISCV::BuildPairF64Pseudo &&		  assert(MI.getOpcode() == RISCV::BuildPairF64Pseudo &&
         "Unexpected instruction");				         "Unexpected instruction");

  MachineFunction &MF = *BB->getParent();			  MachineFunction &MF = *BB->getParent();
  DebugLoc DL = MI.getDebugLoc();				  DebugLoc DL = MI.getDebugLoc();
  const TargetInstrInfo &TII = *MF.getSubtarget().getInstrInf	  const TargetInstrInfo &TII = *MF.getSubtarget().getInstrInf
  const TargetRegisterInfo *RI = MF.getSubtarget().getRegiste	  const TargetRegisterInfo *RI = MF.getSubtarget().getRegiste
  unsigned DstReg = MI.getOperand(0).getReg();			  unsigned DstReg = MI.getOperand(0).getReg();
  unsigned LoReg = MI.getOperand(1).getReg();			  unsigned LoReg = MI.getOperand(1).getReg();
  unsigned HiReg = MI.getOperand(2).getReg();			  unsigned HiReg = MI.getOperand(2).getReg();
  const TargetRegisterClass *DstRC = &RISCV::FPR64RegClass;	  const TargetRegisterClass *DstRC = &RISCV::FPR64RegClass;
  int FI = MF.getInfo<RISCVMachineFunctionInfo>()->getMoveF64	  int FI = MF.getInfo<RISCVMachineFunctionInfo>()->getMoveF64

  MachineMemOperand *MMO =					  MachineMemOperand *MMO =
      MF.getMachineMemOperand(MachinePointerInfo::getFixedSta	      MF.getMachineMemOperand(MachinePointerInfo::getFixedSta
                              MachineMemOperand::MOStore, 8, 	                              MachineMemOperand::MOStore, 8, 
  BuildMI(*BB, MI, DL, TII.get(RISCV::SW))			  BuildMI(*BB, MI, DL, TII.get(RISCV::SW))
      .addReg(LoReg, getKillRegState(MI.getOperand(1).isKill(	      .addReg(LoReg, getKillRegState(MI.getOperand(1).isKill(
      .addFrameIndex(FI)					      .addFrameIndex(FI)
      .addImm(0)						      .addImm(0)
      .addMemOperand(MMO);					      .addMemOperand(MMO);
  BuildMI(*BB, MI, DL, TII.get(RISCV::SW))			  BuildMI(*BB, MI, DL, TII.get(RISCV::SW))
      .addReg(HiReg, getKillRegState(MI.getOperand(2).isKill(	      .addReg(HiReg, getKillRegState(MI.getOperand(2).isKill(
      .addFrameIndex(FI)					      .addFrameIndex(FI)
      .addImm(4)						      .addImm(4)
      .addMemOperand(MMO);					      .addMemOperand(MMO);
  TII.loadRegFromStackSlot(*BB, MI, DstReg, FI, DstRC, RI);	  TII.loadRegFromStackSlot(*BB, MI, DstReg, FI, DstRC, RI);
  MI.eraseFromParent(); // The pseudo instruction is gone now	  MI.eraseFromParent(); // The pseudo instruction is gone now
  return BB;							  return BB;
}								}

MachineBasicBlock *						MachineBasicBlock *
RISCVTargetLowering::EmitInstrWithCustomInserter(MachineInstr	RISCVTargetLowering::EmitInstrWithCustomInserter(MachineInstr
                                                 MachineBasic	                                                 MachineBasic
  switch (MI.getOpcode()) {					  switch (MI.getOpcode()) {
  default:							  default:
    llvm_unreachable("Unexpected instr type to insert");	    llvm_unreachable("Unexpected instr type to insert");
  case RISCV::Select_GPR_Using_CC_GPR:				  case RISCV::Select_GPR_Using_CC_GPR:
  case RISCV::Select_FPR32_Using_CC_GPR:			  case RISCV::Select_FPR32_Using_CC_GPR:
  case RISCV::Select_FPR64_Using_CC_GPR:			  case RISCV::Select_FPR64_Using_CC_GPR:
    break;							    break;
  case RISCV::BuildPairF64Pseudo:				  case RISCV::BuildPairF64Pseudo:
    return emitBuildPairF64Pseudo(MI, BB);			    return emitBuildPairF64Pseudo(MI, BB);
  case RISCV::SplitF64Pseudo:					  case RISCV::SplitF64Pseudo:
    return emitSplitF64Pseudo(MI, BB);				    return emitSplitF64Pseudo(MI, BB);
  }								  }

  // To "insert" a SELECT instruction, we actually have to in	  // To "insert" a SELECT instruction, we actually have to in
  // control-flow pattern.  The incoming instruction knows th	  // control-flow pattern.  The incoming instruction knows th
  // to set, the condition code register to branch on, the tr	  // to set, the condition code register to branch on, the tr
  // select between, and the condcode to use to select the ap	  // select between, and the condcode to use to select the ap
  //								  //
  // We produce the following control flow:			  // We produce the following control flow:
  //     HeadMBB						  //     HeadMBB
  //     |  \							  //     |  \
  //     |  IfFalseMBB						  //     |  IfFalseMBB
  //     | /							  //     | /
  //    TailMBB							  //    TailMBB
  const TargetInstrInfo &TII = *BB->getParent()->getSubtarget	  const TargetInstrInfo &TII = *BB->getParent()->getSubtarget
  const BasicBlock *LLVM_BB = BB->getBasicBlock();		  const BasicBlock *LLVM_BB = BB->getBasicBlock();
  DebugLoc DL = MI.getDebugLoc();				  DebugLoc DL = MI.getDebugLoc();
  MachineFunction::iterator I = ++BB->getIterator();		  MachineFunction::iterator I = ++BB->getIterator();

  MachineBasicBlock *HeadMBB = BB;				  MachineBasicBlock *HeadMBB = BB;
  MachineFunction *F = BB->getParent();				  MachineFunction *F = BB->getParent();
  MachineBasicBlock *TailMBB = F->CreateMachineBasicBlock(LLV	  MachineBasicBlock *TailMBB = F->CreateMachineBasicBlock(LLV
  MachineBasicBlock *IfFalseMBB = F->CreateMachineBasicBlock(	  MachineBasicBlock *IfFalseMBB = F->CreateMachineBasicBlock(

  F->insert(I, IfFalseMBB);					  F->insert(I, IfFalseMBB);
  F->insert(I, TailMBB);					  F->insert(I, TailMBB);
  // Move all remaining instructions to TailMBB.		  // Move all remaining instructions to TailMBB.
  TailMBB->splice(TailMBB->begin(), HeadMBB,			  TailMBB->splice(TailMBB->begin(), HeadMBB,
                  std::next(MachineBasicBlock::iterator(MI)),	                  std::next(MachineBasicBlock::iterator(MI)),
  // Update machine-CFG edges by transferring all successors 	  // Update machine-CFG edges by transferring all successors 
  // block to the new block which will contain the Phi node f	  // block to the new block which will contain the Phi node f
  TailMBB->transferSuccessorsAndUpdatePHIs(HeadMBB);		  TailMBB->transferSuccessorsAndUpdatePHIs(HeadMBB);
  // Set the successors for HeadMBB.				  // Set the successors for HeadMBB.
  HeadMBB->addSuccessor(IfFalseMBB);				  HeadMBB->addSuccessor(IfFalseMBB);
  HeadMBB->addSuccessor(TailMBB);				  HeadMBB->addSuccessor(TailMBB);

  // Insert appropriate branch.					  // Insert appropriate branch.
  unsigned LHS = MI.getOperand(1).getReg();			  unsigned LHS = MI.getOperand(1).getReg();
  unsigned RHS = MI.getOperand(2).getReg();			  unsigned RHS = MI.getOperand(2).getReg();
  auto CC = static_cast<ISD::CondCode>(MI.getOperand(3).getIm	  auto CC = static_cast<ISD::CondCode>(MI.getOperand(3).getIm
  unsigned Opcode = getBranchOpcodeForIntCondCode(CC);		  unsigned Opcode = getBranchOpcodeForIntCondCode(CC);

  BuildMI(HeadMBB, DL, TII.get(Opcode))				  BuildMI(HeadMBB, DL, TII.get(Opcode))
    .addReg(LHS)						    .addReg(LHS)
    .addReg(RHS)						    .addReg(RHS)
    .addMBB(TailMBB);						    .addMBB(TailMBB);

  // IfFalseMBB just falls through to TailMBB.			  // IfFalseMBB just falls through to TailMBB.
  IfFalseMBB->addSuccessor(TailMBB);				  IfFalseMBB->addSuccessor(TailMBB);

  // %Result = phi [ %TrueValue, HeadMBB ], [ %FalseValue, If	  // %Result = phi [ %TrueValue, HeadMBB ], [ %FalseValue, If
  BuildMI(*TailMBB, TailMBB->begin(), DL, TII.get(RISCV::PHI)	  BuildMI(*TailMBB, TailMBB->begin(), DL, TII.get(RISCV::PHI)
          MI.getOperand(0).getReg())				          MI.getOperand(0).getReg())
      .addReg(MI.getOperand(4).getReg())			      .addReg(MI.getOperand(4).getReg())
      .addMBB(HeadMBB)						      .addMBB(HeadMBB)
      .addReg(MI.getOperand(5).getReg())			      .addReg(MI.getOperand(5).getReg())
      .addMBB(IfFalseMBB);					      .addMBB(IfFalseMBB);

  MI.eraseFromParent(); // The pseudo instruction is gone now	  MI.eraseFromParent(); // The pseudo instruction is gone now
  return TailMBB;						  return TailMBB;
}								}

// Calling Convention Implementation.				// Calling Convention Implementation.
// The expectations for frontend ABI lowering vary from targe	// The expectations for frontend ABI lowering vary from targe
// Ideally, an LLVM frontend would be able to avoid worrying 	// Ideally, an LLVM frontend would be able to avoid worrying 
// details, but this is a longer term goal. For now, we simpl	// details, but this is a longer term goal. For now, we simpl
// role of the frontend as simple and well-defined as possibl	// role of the frontend as simple and well-defined as possibl
// be summarised as:						// be summarised as:
// * Never split up large scalar arguments. We handle them he	// * Never split up large scalar arguments. We handle them he
// * If a hardfloat calling convention is being used, and the	// * If a hardfloat calling convention is being used, and the
// passed in a pair of registers (fp+fp, int+fp), and both re	// passed in a pair of registers (fp+fp, int+fp), and both re
// available, then pass as two separate arguments. If either 	// available, then pass as two separate arguments. If either 
// are exhausted, then pass according to the rule below.	// are exhausted, then pass according to the rule below.
// * If a struct could never be passed in registers or direct	// * If a struct could never be passed in registers or direct
// slot (as it is larger than 2*XLEN and the floating point r	// slot (as it is larger than 2*XLEN and the floating point r
// apply), then pass it using a pointer with the byval attrib	// apply), then pass it using a pointer with the byval attrib
// * If a struct is less than 2*XLEN, then coerce to either a	// * If a struct is less than 2*XLEN, then coerce to either a
// word-sized array or a 2*XLEN scalar (depending on alignmen	// word-sized array or a 2*XLEN scalar (depending on alignmen
// * The frontend can determine whether a struct is returned 	// * The frontend can determine whether a struct is returned 
// not based on its size and fields. If it will be returned b	// not based on its size and fields. If it will be returned b
// frontend must modify the prototype so a pointer with the s	// frontend must modify the prototype so a pointer with the s
// passed as the first argument. This is not necessary for la	// passed as the first argument. This is not necessary for la
// returns.							// returns.
// * Struct return values and varargs should be coerced to st	// * Struct return values and varargs should be coerced to st
// register-size fields in the same situations they would be 	// register-size fields in the same situations they would be 
// arguments.							// arguments.

static const MCPhysReg ArgGPRs[] = {				static const MCPhysReg ArgGPRs[] = {
  RISCV::X10, RISCV::X11, RISCV::X12, RISCV::X13,		  RISCV::X10, RISCV::X11, RISCV::X12, RISCV::X13,
  RISCV::X14, RISCV::X15, RISCV::X16, RISCV::X17		  RISCV::X14, RISCV::X15, RISCV::X16, RISCV::X17
};								};

// Pass a 2*XLEN argument that has been split into two XLEN v	// Pass a 2*XLEN argument that has been split into two XLEN v
// registers or the stack as necessary.				// registers or the stack as necessary.
static bool CC_RISCVAssign2XLen(unsigned XLen, CCState &State	static bool CC_RISCVAssign2XLen(unsigned XLen, CCState &State
                                ISD::ArgFlagsTy ArgFlags1, un	                                ISD::ArgFlagsTy ArgFlags1, un
                                MVT ValVT2, MVT LocVT2,		                                MVT ValVT2, MVT LocVT2,
                                ISD::ArgFlagsTy ArgFlags2) {	                                ISD::ArgFlagsTy ArgFlags2) {
  unsigned XLenInBytes = XLen / 8;				  unsigned XLenInBytes = XLen / 8;
  if (unsigned Reg = State.AllocateReg(ArgGPRs)) {		  if (unsigned Reg = State.AllocateReg(ArgGPRs)) {
    // At least one half can be passed via register.		    // At least one half can be passed via register.
    State.addLoc(CCValAssign::getReg(VA1.getValNo(), VA1.getV	    State.addLoc(CCValAssign::getReg(VA1.getValNo(), VA1.getV
                                     VA1.getLocVT(), CCValAss	                                     VA1.getLocVT(), CCValAss
  } else {							  } else {
    // Both halves must be passed on the stack, with proper a	    // Both halves must be passed on the stack, with proper a
    unsigned StackAlign = std::max(XLenInBytes, ArgFlags1.get	    unsigned StackAlign = std::max(XLenInBytes, ArgFlags1.get
    State.addLoc(						    State.addLoc(
        CCValAssign::getMem(VA1.getValNo(), VA1.getValVT(),	        CCValAssign::getMem(VA1.getValNo(), VA1.getValVT(),
                            State.AllocateStack(XLenInBytes, 	                            State.AllocateStack(XLenInBytes, 
                            VA1.getLocVT(), CCValAssign::Full	                            VA1.getLocVT(), CCValAssign::Full
    State.addLoc(CCValAssign::getMem(				    State.addLoc(CCValAssign::getMem(
        ValNo2, ValVT2, State.AllocateStack(XLenInBytes, XLen	        ValNo2, ValVT2, State.AllocateStack(XLenInBytes, XLen
        CCValAssign::Full));					        CCValAssign::Full));
    return false;						    return false;
  }								  }

  if (unsigned Reg = State.AllocateReg(ArgGPRs)) {		  if (unsigned Reg = State.AllocateReg(ArgGPRs)) {
    // The second half can also be passed via register.		    // The second half can also be passed via register.
    State.addLoc(						    State.addLoc(
        CCValAssign::getReg(ValNo2, ValVT2, Reg, LocVT2, CCVa	        CCValAssign::getReg(ValNo2, ValVT2, Reg, LocVT2, CCVa
  } else {							  } else {
    // The second half is passed via the stack, without addit	    // The second half is passed via the stack, without addit
    State.addLoc(CCValAssign::getMem(				    State.addLoc(CCValAssign::getMem(
        ValNo2, ValVT2, State.AllocateStack(XLenInBytes, XLen	        ValNo2, ValVT2, State.AllocateStack(XLenInBytes, XLen
        CCValAssign::Full));					        CCValAssign::Full));
  }								  }

  return false;							  return false;
}								}

// Implements the RISC-V calling convention. Returns true upo	// Implements the RISC-V calling convention. Returns true upo
static bool CC_RISCV(const DataLayout &DL, unsigned ValNo, MV	static bool CC_RISCV(const DataLayout &DL, unsigned ValNo, MV
                     CCValAssign::LocInfo LocInfo, ISD::ArgFl	                     CCValAssign::LocInfo LocInfo, ISD::ArgFl
                     CCState &State, bool IsFixed, bool IsRet	                     CCState &State, bool IsFixed, bool IsRet
  unsigned XLen = DL.getLargestLegalIntTypeSizeInBits();	  unsigned XLen = DL.getLargestLegalIntTypeSizeInBits();
  assert(XLen == 32 || XLen == 64);				  assert(XLen == 32 || XLen == 64);
  MVT XLenVT = XLen == 32 ? MVT::i32 : MVT::i64;		  MVT XLenVT = XLen == 32 ? MVT::i32 : MVT::i64;
  if (ValVT == MVT::f32) {					  if (ValVT == MVT::f32) {
    LocVT = MVT::i32;						    LocVT = MVT::i32;
    LocInfo = CCValAssign::BCvt;				    LocInfo = CCValAssign::BCvt;
  }								  }

  // Any return value split in to more than two values can't 	  // Any return value split in to more than two values can't 
  // directly.							  // directly.
  if (IsRet && ValNo > 1)					  if (IsRet && ValNo > 1)
    return true;						    return true;

  // If this is a variadic argument, the RISC-V calling conve	  // If this is a variadic argument, the RISC-V calling conve
  // that it is assigned an 'even' or 'aligned' register if i	  // that it is assigned an 'even' or 'aligned' register if i
  // alignment (RV32) or 16-byte alignment (RV64). An aligned	  // alignment (RV32) or 16-byte alignment (RV64). An aligned
  // be used regardless of whether the original argument was 	  // be used regardless of whether the original argument was 
  // legalisation or not. The argument will not be passed by 	  // legalisation or not. The argument will not be passed by 
  // original type is larger than 2*XLEN, so the register ali	  // original type is larger than 2*XLEN, so the register ali
  // not apply.							  // not apply.
  unsigned TwoXLenInBytes = (2 * XLen) / 8;			  unsigned TwoXLenInBytes = (2 * XLen) / 8;
  if (!IsFixed && ArgFlags.getOrigAlign() == TwoXLenInBytes &	  if (!IsFixed && ArgFlags.getOrigAlign() == TwoXLenInBytes &
      DL.getTypeAllocSize(OrigTy) == TwoXLenInBytes) {		      DL.getTypeAllocSize(OrigTy) == TwoXLenInBytes) {
    unsigned RegIdx = State.getFirstUnallocated(ArgGPRs);	    unsigned RegIdx = State.getFirstUnallocated(ArgGPRs);
    // Skip 'odd' register if necessary.			    // Skip 'odd' register if necessary.
    if (RegIdx != array_lengthof(ArgGPRs) && RegIdx % 2 == 1)	    if (RegIdx != array_lengthof(ArgGPRs) && RegIdx % 2 == 1)
      State.AllocateReg(ArgGPRs);				      State.AllocateReg(ArgGPRs);
  }								  }

  SmallVectorImpl<CCValAssign> &PendingLocs = State.getPendin	  SmallVectorImpl<CCValAssign> &PendingLocs = State.getPendin
  SmallVectorImpl<ISD::ArgFlagsTy> &PendingArgFlags =		  SmallVectorImpl<ISD::ArgFlagsTy> &PendingArgFlags =
      State.getPendingArgFlags();				      State.getPendingArgFlags();

  assert(PendingLocs.size() == PendingArgFlags.size() &&	  assert(PendingLocs.size() == PendingArgFlags.size() &&
         "PendingLocs and PendingArgFlags out of sync");	         "PendingLocs and PendingArgFlags out of sync");

  // Handle passing f64 on RV32D with a soft float ABI.		  // Handle passing f64 on RV32D with a soft float ABI.
  if (XLen == 32 && ValVT == MVT::f64) {			  if (XLen == 32 && ValVT == MVT::f64) {
    assert(!ArgFlags.isSplit() && PendingLocs.empty() &&	    assert(!ArgFlags.isSplit() && PendingLocs.empty() &&
           "Can't lower f64 if it is split");			           "Can't lower f64 if it is split");
    // Depending on available argument GPRS, f64 may be passe	    // Depending on available argument GPRS, f64 may be passe
    // GPRs, split between a GPR and the stack, or passed com	    // GPRs, split between a GPR and the stack, or passed com
    // stack. LowerCall/LowerFormalArguments/LowerReturn must	    // stack. LowerCall/LowerFormalArguments/LowerReturn must
    // cases.							    // cases.
    unsigned Reg = State.AllocateReg(ArgGPRs);			    unsigned Reg = State.AllocateReg(ArgGPRs);
    LocVT = MVT::i32;						    LocVT = MVT::i32;
    if (!Reg) {							    if (!Reg) {
      unsigned StackOffset = State.AllocateStack(8, 8);		      unsigned StackOffset = State.AllocateStack(8, 8);
      State.addLoc(						      State.addLoc(
          CCValAssign::getMem(ValNo, ValVT, StackOffset, LocV	          CCValAssign::getMem(ValNo, ValVT, StackOffset, LocV
      return false;						      return false;
    }								    }
    if (!State.AllocateReg(ArgGPRs))				    if (!State.AllocateReg(ArgGPRs))
      State.AllocateStack(4, 4);				      State.AllocateStack(4, 4);
    State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT	    State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT
    return false;						    return false;
  }								  }

  // Split arguments might be passed indirectly, so keep trac	  // Split arguments might be passed indirectly, so keep trac
  // values.							  // values.
  if (ArgFlags.isSplit() || !PendingLocs.empty()) {		  if (ArgFlags.isSplit() || !PendingLocs.empty()) {
    LocVT = XLenVT;						    LocVT = XLenVT;
    LocInfo = CCValAssign::Indirect;				    LocInfo = CCValAssign::Indirect;
    PendingLocs.push_back(					    PendingLocs.push_back(
        CCValAssign::getPending(ValNo, ValVT, LocVT, LocInfo)	        CCValAssign::getPending(ValNo, ValVT, LocVT, LocInfo)
    PendingArgFlags.push_back(ArgFlags);			    PendingArgFlags.push_back(ArgFlags);
    if (!ArgFlags.isSplitEnd()) {				    if (!ArgFlags.isSplitEnd()) {
      return false;						      return false;
    }								    }
  }								  }

  // If the split argument only had two elements, it should b	  // If the split argument only had two elements, it should b
  // in registers or on the stack.				  // in registers or on the stack.
  if (ArgFlags.isSplitEnd() && PendingLocs.size() <= 2) {	  if (ArgFlags.isSplitEnd() && PendingLocs.size() <= 2) {
    assert(PendingLocs.size() == 2 && "Unexpected PendingLocs	    assert(PendingLocs.size() == 2 && "Unexpected PendingLocs
    // Apply the normal calling convention rules to the first	    // Apply the normal calling convention rules to the first
    // split argument.						    // split argument.
    CCValAssign VA = PendingLocs[0];				    CCValAssign VA = PendingLocs[0];
    ISD::ArgFlagsTy AF = PendingArgFlags[0];			    ISD::ArgFlagsTy AF = PendingArgFlags[0];
    PendingLocs.clear();					    PendingLocs.clear();
    PendingArgFlags.clear();					    PendingArgFlags.clear();
    return CC_RISCVAssign2XLen(XLen, State, VA, AF, ValNo, Va	    return CC_RISCVAssign2XLen(XLen, State, VA, AF, ValNo, Va
                               ArgFlags);			                               ArgFlags);
  }								  }

  // Allocate to a register if possible, or else a stack slot	  // Allocate to a register if possible, or else a stack slot
  unsigned Reg = State.AllocateReg(ArgGPRs);			  unsigned Reg = State.AllocateReg(ArgGPRs);
  unsigned StackOffset = Reg ? 0 : State.AllocateStack(XLen /	  unsigned StackOffset = Reg ? 0 : State.AllocateStack(XLen /

  // If we reach this point and PendingLocs is non-empty, we 	  // If we reach this point and PendingLocs is non-empty, we 
  // end of a split argument that must be passed indirectly.	  // end of a split argument that must be passed indirectly.
  if (!PendingLocs.empty()) {					  if (!PendingLocs.empty()) {
    assert(ArgFlags.isSplitEnd() && "Expected ArgFlags.isSpli	    assert(ArgFlags.isSplitEnd() && "Expected ArgFlags.isSpli
    assert(PendingLocs.size() > 2 && "Unexpected PendingLocs.	    assert(PendingLocs.size() > 2 && "Unexpected PendingLocs.

    for (auto &It : PendingLocs) {				    for (auto &It : PendingLocs) {
      if (Reg)							      if (Reg)
        It.convertToReg(Reg);					        It.convertToReg(Reg);
      else							      else
        It.convertToMem(StackOffset);				        It.convertToMem(StackOffset);
      State.addLoc(It);						      State.addLoc(It);
    }								    }
    PendingLocs.clear();					    PendingLocs.clear();
    PendingArgFlags.clear();					    PendingArgFlags.clear();
    return false;						    return false;
  }								  }

  assert(LocVT == XLenVT && "Expected an XLenVT at this stage	  assert(LocVT == XLenVT && "Expected an XLenVT at this stage

  if (Reg) {							  if (Reg) {
    State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT	    State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT
    return false;						    return false;
  }								  }

  if (ValVT == MVT::f32) {					  if (ValVT == MVT::f32) {
    LocVT = MVT::f32;						    LocVT = MVT::f32;
    LocInfo = CCValAssign::Full;				    LocInfo = CCValAssign::Full;
  }								  }
  State.addLoc(CCValAssign::getMem(ValNo, ValVT, StackOffset,	  State.addLoc(CCValAssign::getMem(ValNo, ValVT, StackOffset,
  return false;							  return false;
}								}

void RISCVTargetLowering::analyzeInputArgs(			void RISCVTargetLowering::analyzeInputArgs(
    MachineFunction &MF, CCState &CCInfo,			    MachineFunction &MF, CCState &CCInfo,
    const SmallVectorImpl<ISD::InputArg> &Ins, bool IsRet) co	    const SmallVectorImpl<ISD::InputArg> &Ins, bool IsRet) co
  unsigned NumArgs = Ins.size();				  unsigned NumArgs = Ins.size();
  FunctionType *FType = MF.getFunction().getFunctionType();	  FunctionType *FType = MF.getFunction().getFunctionType();

  for (unsigned i = 0; i != NumArgs; ++i) {			  for (unsigned i = 0; i != NumArgs; ++i) {
    MVT ArgVT = Ins[i].VT;					    MVT ArgVT = Ins[i].VT;
    ISD::ArgFlagsTy ArgFlags = Ins[i].Flags;			    ISD::ArgFlagsTy ArgFlags = Ins[i].Flags;

    Type *ArgTy = nullptr;					    Type *ArgTy = nullptr;
    if (IsRet)							    if (IsRet)
      ArgTy = FType->getReturnType();				      ArgTy = FType->getReturnType();
    else if (Ins[i].isOrigArg())				    else if (Ins[i].isOrigArg())
      ArgTy = FType->getParamType(Ins[i].getOrigArgIndex());	      ArgTy = FType->getParamType(Ins[i].getOrigArgIndex());

    if (CC_RISCV(MF.getDataLayout(), i, ArgVT, ArgVT, CCValAs	    if (CC_RISCV(MF.getDataLayout(), i, ArgVT, ArgVT, CCValAs
                 ArgFlags, CCInfo, /*IsRet=*/true, IsRet, Arg	                 ArgFlags, CCInfo, /*IsRet=*/true, IsRet, Arg
      LLVM_DEBUG(dbgs() << "InputArg #" << i << " has unhandl	      LLVM_DEBUG(dbgs() << "InputArg #" << i << " has unhandl
                        << EVT(ArgVT).getEVTString() << '\n')	                        << EVT(ArgVT).getEVTString() << '\n')
      llvm_unreachable(nullptr);				      llvm_unreachable(nullptr);
    }								    }
  }								  }
}								}

void RISCVTargetLowering::analyzeOutputArgs(			void RISCVTargetLowering::analyzeOutputArgs(
    MachineFunction &MF, CCState &CCInfo,			    MachineFunction &MF, CCState &CCInfo,
    const SmallVectorImpl<ISD::OutputArg> &Outs, bool IsRet,	    const SmallVectorImpl<ISD::OutputArg> &Outs, bool IsRet,
    CallLoweringInfo *CLI) const {				    CallLoweringInfo *CLI) const {
  unsigned NumArgs = Outs.size();				  unsigned NumArgs = Outs.size();

  for (unsigned i = 0; i != NumArgs; i++) {			  for (unsigned i = 0; i != NumArgs; i++) {
    MVT ArgVT = Outs[i].VT;					    MVT ArgVT = Outs[i].VT;
    ISD::ArgFlagsTy ArgFlags = Outs[i].Flags;			    ISD::ArgFlagsTy ArgFlags = Outs[i].Flags;
    Type *OrigTy = CLI ? CLI->getArgs()[Outs[i].OrigArgIndex]	    Type *OrigTy = CLI ? CLI->getArgs()[Outs[i].OrigArgIndex]

    if (CC_RISCV(MF.getDataLayout(), i, ArgVT, ArgVT, CCValAs	    if (CC_RISCV(MF.getDataLayout(), i, ArgVT, ArgVT, CCValAs
                 ArgFlags, CCInfo, Outs[i].IsFixed, IsRet, Or	                 ArgFlags, CCInfo, Outs[i].IsFixed, IsRet, Or
      LLVM_DEBUG(dbgs() << "OutputArg #" << i << " has unhand	      LLVM_DEBUG(dbgs() << "OutputArg #" << i << " has unhand
                        << EVT(ArgVT).getEVTString() << "\n")	                        << EVT(ArgVT).getEVTString() << "\n")
      llvm_unreachable(nullptr);				      llvm_unreachable(nullptr);
    }								    }
  }								  }
}								}

// Convert Val to a ValVT. Should not be called for CCValAssi	// Convert Val to a ValVT. Should not be called for CCValAssi
// values.							// values.
static SDValue convertLocVTToValVT(SelectionDAG &DAG, SDValue	static SDValue convertLocVTToValVT(SelectionDAG &DAG, SDValue
                                   const CCValAssign &VA, con	                                   const CCValAssign &VA, con
  switch (VA.getLocInfo()) {					  switch (VA.getLocInfo()) {
  default:							  default:
    llvm_unreachable("Unexpected CCValAssign::LocInfo");	    llvm_unreachable("Unexpected CCValAssign::LocInfo");
  case CCValAssign::Full:					  case CCValAssign::Full:
    break;							    break;
  case CCValAssign::BCvt:					  case CCValAssign::BCvt:
    Val = DAG.getNode(ISD::BITCAST, DL, VA.getValVT(), Val);	    Val = DAG.getNode(ISD::BITCAST, DL, VA.getValVT(), Val);
    break;							    break;
  }								  }
  return Val;							  return Val;
}								}

// The caller is responsible for loading the full value if th	// The caller is responsible for loading the full value if th
// passed with CCValAssign::Indirect.				// passed with CCValAssign::Indirect.
static SDValue unpackFromRegLoc(SelectionDAG &DAG, SDValue Ch	static SDValue unpackFromRegLoc(SelectionDAG &DAG, SDValue Ch
                                const CCValAssign &VA, const 	                                const CCValAssign &VA, const 
  MachineFunction &MF = DAG.getMachineFunction();		  MachineFunction &MF = DAG.getMachineFunction();
  MachineRegisterInfo &RegInfo = MF.getRegInfo();		  MachineRegisterInfo &RegInfo = MF.getRegInfo();
  EVT LocVT = VA.getLocVT();					  EVT LocVT = VA.getLocVT();
  SDValue Val;							  SDValue Val;

  unsigned VReg = RegInfo.createVirtualRegister(&RISCV::GPRRe	  unsigned VReg = RegInfo.createVirtualRegister(&RISCV::GPRRe
  RegInfo.addLiveIn(VA.getLocReg(), VReg);			  RegInfo.addLiveIn(VA.getLocReg(), VReg);
  Val = DAG.getCopyFromReg(Chain, DL, VReg, LocVT);		  Val = DAG.getCopyFromReg(Chain, DL, VReg, LocVT);

  if (VA.getLocInfo() == CCValAssign::Indirect)			  if (VA.getLocInfo() == CCValAssign::Indirect)
    return Val;							    return Val;

  return convertLocVTToValVT(DAG, Val, VA, DL);			  return convertLocVTToValVT(DAG, Val, VA, DL);
}								}

static SDValue convertValVTToLocVT(SelectionDAG &DAG, SDValue	static SDValue convertValVTToLocVT(SelectionDAG &DAG, SDValue
                                   const CCValAssign &VA, con	                                   const CCValAssign &VA, con
  EVT LocVT = VA.getLocVT();					  EVT LocVT = VA.getLocVT();

  switch (VA.getLocInfo()) {					  switch (VA.getLocInfo()) {
  default:							  default:
    llvm_unreachable("Unexpected CCValAssign::LocInfo");	    llvm_unreachable("Unexpected CCValAssign::LocInfo");
  case CCValAssign::Full:					  case CCValAssign::Full:
    break;							    break;
  case CCValAssign::BCvt:					  case CCValAssign::BCvt:
    Val = DAG.getNode(ISD::BITCAST, DL, LocVT, Val);		    Val = DAG.getNode(ISD::BITCAST, DL, LocVT, Val);
    break;							    break;
  }								  }
  return Val;							  return Val;
}								}

// The caller is responsible for loading the full value if th	// The caller is responsible for loading the full value if th
// passed with CCValAssign::Indirect.				// passed with CCValAssign::Indirect.
static SDValue unpackFromMemLoc(SelectionDAG &DAG, SDValue Ch	static SDValue unpackFromMemLoc(SelectionDAG &DAG, SDValue Ch
                                const CCValAssign &VA, const 	                                const CCValAssign &VA, const 
  MachineFunction &MF = DAG.getMachineFunction();		  MachineFunction &MF = DAG.getMachineFunction();
  MachineFrameInfo &MFI = MF.getFrameInfo();			  MachineFrameInfo &MFI = MF.getFrameInfo();
  EVT LocVT = VA.getLocVT();					  EVT LocVT = VA.getLocVT();
  EVT ValVT = VA.getValVT();					  EVT ValVT = VA.getValVT();
  EVT PtrVT = MVT::getIntegerVT(DAG.getDataLayout().getPointe	  EVT PtrVT = MVT::getIntegerVT(DAG.getDataLayout().getPointe
  int FI = MFI.CreateFixedObject(ValVT.getSizeInBits() / 8,	  int FI = MFI.CreateFixedObject(ValVT.getSizeInBits() / 8,
                                 VA.getLocMemOffset(), /*Immu	                                 VA.getLocMemOffset(), /*Immu
  SDValue FIN = DAG.getFrameIndex(FI, PtrVT);			  SDValue FIN = DAG.getFrameIndex(FI, PtrVT);
  SDValue Val;							  SDValue Val;

  ISD::LoadExtType ExtType;					  ISD::LoadExtType ExtType;
  switch (VA.getLocInfo()) {					  switch (VA.getLocInfo()) {
  default:							  default:
    llvm_unreachable("Unexpected CCValAssign::LocInfo");	    llvm_unreachable("Unexpected CCValAssign::LocInfo");
  case CCValAssign::Full:					  case CCValAssign::Full:
  case CCValAssign::Indirect:					  case CCValAssign::Indirect:
    ExtType = ISD::NON_EXTLOAD;					    ExtType = ISD::NON_EXTLOAD;
    break;							    break;
  }								  }
  Val = DAG.getExtLoad(						  Val = DAG.getExtLoad(
      ExtType, DL, LocVT, Chain, FIN,				      ExtType, DL, LocVT, Chain, FIN,
      MachinePointerInfo::getFixedStack(DAG.getMachineFunctio	      MachinePointerInfo::getFixedStack(DAG.getMachineFunctio
  return Val;							  return Val;
}								}

static SDValue unpackF64OnRV32DSoftABI(SelectionDAG &DAG, SDV	static SDValue unpackF64OnRV32DSoftABI(SelectionDAG &DAG, SDV
                                       const CCValAssign &VA,	                                       const CCValAssign &VA,
  assert(VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f	  assert(VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f
         "Unexpected VA");					         "Unexpected VA");
  MachineFunction &MF = DAG.getMachineFunction();		  MachineFunction &MF = DAG.getMachineFunction();
  MachineFrameInfo &MFI = MF.getFrameInfo();			  MachineFrameInfo &MFI = MF.getFrameInfo();
  MachineRegisterInfo &RegInfo = MF.getRegInfo();		  MachineRegisterInfo &RegInfo = MF.getRegInfo();

  if (VA.isMemLoc()) {						  if (VA.isMemLoc()) {
    // f64 is passed on the stack.				    // f64 is passed on the stack.
    int FI = MFI.CreateFixedObject(8, VA.getLocMemOffset(), /	    int FI = MFI.CreateFixedObject(8, VA.getLocMemOffset(), /
    SDValue FIN = DAG.getFrameIndex(FI, MVT::i32);		    SDValue FIN = DAG.getFrameIndex(FI, MVT::i32);
    return DAG.getLoad(MVT::f64, DL, Chain, FIN,		    return DAG.getLoad(MVT::f64, DL, Chain, FIN,
                       MachinePointerInfo::getFixedStack(MF, 	                       MachinePointerInfo::getFixedStack(MF, 
  }								  }

  assert(VA.isRegLoc() && "Expected register VA assignment");	  assert(VA.isRegLoc() && "Expected register VA assignment");

  unsigned LoVReg = RegInfo.createVirtualRegister(&RISCV::GPR	  unsigned LoVReg = RegInfo.createVirtualRegister(&RISCV::GPR
  RegInfo.addLiveIn(VA.getLocReg(), LoVReg);			  RegInfo.addLiveIn(VA.getLocReg(), LoVReg);
  SDValue Lo = DAG.getCopyFromReg(Chain, DL, LoVReg, MVT::i32	  SDValue Lo = DAG.getCopyFromReg(Chain, DL, LoVReg, MVT::i32
  SDValue Hi;							  SDValue Hi;
  if (VA.getLocReg() == RISCV::X17) {				  if (VA.getLocReg() == RISCV::X17) {
    // Second half of f64 is passed on the stack.		    // Second half of f64 is passed on the stack.
    int FI = MFI.CreateFixedObject(4, 0, /*Immutable=*/true);	    int FI = MFI.CreateFixedObject(4, 0, /*Immutable=*/true);
    SDValue FIN = DAG.getFrameIndex(FI, MVT::i32);		    SDValue FIN = DAG.getFrameIndex(FI, MVT::i32);
    Hi = DAG.getLoad(MVT::i32, DL, Chain, FIN,			    Hi = DAG.getLoad(MVT::i32, DL, Chain, FIN,
                     MachinePointerInfo::getFixedStack(MF, FI	                     MachinePointerInfo::getFixedStack(MF, FI
  } else {							  } else {
    // Second half of f64 is passed in another GPR.		    // Second half of f64 is passed in another GPR.
    unsigned HiVReg = RegInfo.createVirtualRegister(&RISCV::G	    unsigned HiVReg = RegInfo.createVirtualRegister(&RISCV::G
    RegInfo.addLiveIn(VA.getLocReg() + 1, HiVReg);		    RegInfo.addLiveIn(VA.getLocReg() + 1, HiVReg);
    Hi = DAG.getCopyFromReg(Chain, DL, HiVReg, MVT::i32);	    Hi = DAG.getCopyFromReg(Chain, DL, HiVReg, MVT::i32);
  }								  }
  return DAG.getNode(RISCVISD::BuildPairF64, DL, MVT::f64, Lo	  return DAG.getNode(RISCVISD::BuildPairF64, DL, MVT::f64, Lo
}								}

// Transform physical registers into virtual registers.		// Transform physical registers into virtual registers.
SDValue RISCVTargetLowering::LowerFormalArguments(		SDValue RISCVTargetLowering::LowerFormalArguments(
    SDValue Chain, CallingConv::ID CallConv, bool IsVarArg,	    SDValue Chain, CallingConv::ID CallConv, bool IsVarArg,
    const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &D	    const SmallVectorImpl<ISD::InputArg> &Ins, const SDLoc &D
    SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals) cons	    SelectionDAG &DAG, SmallVectorImpl<SDValue> &InVals) cons

  switch (CallConv) {						  switch (CallConv) {
  default:							  default:
    report_fatal_error("Unsupported calling convention");	    report_fatal_error("Unsupported calling convention");
  case CallingConv::C:						  case CallingConv::C:
  case CallingConv::Fast:					  case CallingConv::Fast:
    break;							    break;
  }								  }

  MachineFunction &MF = DAG.getMachineFunction();		  MachineFunction &MF = DAG.getMachineFunction();

  const Function &Func = MF.getFunction();			  const Function &Func = MF.getFunction();
  if (Func.hasFnAttribute("interrupt")) {			  if (Func.hasFnAttribute("interrupt")) {
    if (!Func.arg_empty())					    if (!Func.arg_empty())
      report_fatal_error(					      report_fatal_error(
        "Functions with the interrupt attribute cannot have a	        "Functions with the interrupt attribute cannot have a

    StringRef Kind =						    StringRef Kind =
      MF.getFunction().getFnAttribute("interrupt").getValueAs	      MF.getFunction().getFnAttribute("interrupt").getValueAs

    if (!(Kind == "user" || Kind == "supervisor" || Kind == "	    if (!(Kind == "user" || Kind == "supervisor" || Kind == "
      report_fatal_error(					      report_fatal_error(
        "Function interrupt attribute argument not supported!	        "Function interrupt attribute argument not supported!
  }								  }

  EVT PtrVT = getPointerTy(DAG.getDataLayout());		  EVT PtrVT = getPointerTy(DAG.getDataLayout());
  MVT XLenVT = Subtarget.getXLenVT();				  MVT XLenVT = Subtarget.getXLenVT();
  unsigned XLenInBytes = Subtarget.getXLen() / 8;		  unsigned XLenInBytes = Subtarget.getXLen() / 8;
  // Used with vargs to acumulate store chains.			  // Used with vargs to acumulate store chains.
  std::vector<SDValue> OutChains;				  std::vector<SDValue> OutChains;

  // Assign locations to all of the incoming arguments.		  // Assign locations to all of the incoming arguments.
  SmallVector<CCValAssign, 16> ArgLocs;				  SmallVector<CCValAssign, 16> ArgLocs;
  CCState CCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.getCon	  CCState CCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.getCon
  analyzeInputArgs(MF, CCInfo, Ins, /*IsRet=*/false);		  analyzeInputArgs(MF, CCInfo, Ins, /*IsRet=*/false);

  for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {	  for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {
    CCValAssign &VA = ArgLocs[i];				    CCValAssign &VA = ArgLocs[i];
    SDValue ArgValue;						    SDValue ArgValue;
    // Passing f64 on RV32D with a soft float ABI must be han	    // Passing f64 on RV32D with a soft float ABI must be han
    // case.							    // case.
    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f6	    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f6
      ArgValue = unpackF64OnRV32DSoftABI(DAG, Chain, VA, DL);	      ArgValue = unpackF64OnRV32DSoftABI(DAG, Chain, VA, DL);
    else if (VA.isRegLoc())					    else if (VA.isRegLoc())
      ArgValue = unpackFromRegLoc(DAG, Chain, VA, DL);		      ArgValue = unpackFromRegLoc(DAG, Chain, VA, DL);
    else							    else
      ArgValue = unpackFromMemLoc(DAG, Chain, VA, DL);		      ArgValue = unpackFromMemLoc(DAG, Chain, VA, DL);

    if (VA.getLocInfo() == CCValAssign::Indirect) {		    if (VA.getLocInfo() == CCValAssign::Indirect) {
      // If the original argument was split and passed by ref	      // If the original argument was split and passed by ref
      // on RV32), we need to load all parts of it here (usin	      // on RV32), we need to load all parts of it here (usin
      // address).						      // address).
      InVals.push_back(DAG.getLoad(VA.getValVT(), DL, Chain, 	      InVals.push_back(DAG.getLoad(VA.getValVT(), DL, Chain, 
                                   MachinePointerInfo()));	                                   MachinePointerInfo()));
      unsigned ArgIndex = Ins[i].OrigArgIndex;			      unsigned ArgIndex = Ins[i].OrigArgIndex;
      assert(Ins[i].PartOffset == 0);				      assert(Ins[i].PartOffset == 0);
      while (i + 1 != e && Ins[i + 1].OrigArgIndex == ArgInde	      while (i + 1 != e && Ins[i + 1].OrigArgIndex == ArgInde
        CCValAssign &PartVA = ArgLocs[i + 1];			        CCValAssign &PartVA = ArgLocs[i + 1];
        unsigned PartOffset = Ins[i + 1].PartOffset;		        unsigned PartOffset = Ins[i + 1].PartOffset;
        SDValue Address = DAG.getNode(ISD::ADD, DL, PtrVT, Ar	        SDValue Address = DAG.getNode(ISD::ADD, DL, PtrVT, Ar
                                      DAG.getIntPtrConstant(P	                                      DAG.getIntPtrConstant(P
        InVals.push_back(DAG.getLoad(PartVA.getValVT(), DL, C	        InVals.push_back(DAG.getLoad(PartVA.getValVT(), DL, C
                                     MachinePointerInfo()));	                                     MachinePointerInfo()));
        ++i;							        ++i;
      }								      }
      continue;							      continue;
    }								    }
    InVals.push_back(ArgValue);					    InVals.push_back(ArgValue);
  }								  }

  if (IsVarArg) {						  if (IsVarArg) {
    ArrayRef<MCPhysReg> ArgRegs = makeArrayRef(ArgGPRs);	    ArrayRef<MCPhysReg> ArgRegs = makeArrayRef(ArgGPRs);
    unsigned Idx = CCInfo.getFirstUnallocated(ArgRegs);		    unsigned Idx = CCInfo.getFirstUnallocated(ArgRegs);
    const TargetRegisterClass *RC = &RISCV::GPRRegClass;	    const TargetRegisterClass *RC = &RISCV::GPRRegClass;
    MachineFrameInfo &MFI = MF.getFrameInfo();			    MachineFrameInfo &MFI = MF.getFrameInfo();
    MachineRegisterInfo &RegInfo = MF.getRegInfo();		    MachineRegisterInfo &RegInfo = MF.getRegInfo();
    RISCVMachineFunctionInfo *RVFI = MF.getInfo<RISCVMachineF	    RISCVMachineFunctionInfo *RVFI = MF.getInfo<RISCVMachineF

    // Offset of the first variable argument from stack point	    // Offset of the first variable argument from stack point
    // the vararg save area. For now, the varargs save area i	    // the vararg save area. For now, the varargs save area i
    // large enough to hold a0-a7.				    // large enough to hold a0-a7.
    int VaArgOffset, VarArgsSaveSize;				    int VaArgOffset, VarArgsSaveSize;

    // If all registers are allocated, then all varargs must 	    // If all registers are allocated, then all varargs must 
    // stack and we don't need to save any argregs.		    // stack and we don't need to save any argregs.
    if (ArgRegs.size() == Idx) {				    if (ArgRegs.size() == Idx) {
      VaArgOffset = CCInfo.getNextStackOffset();		      VaArgOffset = CCInfo.getNextStackOffset();
      VarArgsSaveSize = 0;					      VarArgsSaveSize = 0;
    } else {							    } else {
      VarArgsSaveSize = XLenInBytes * (ArgRegs.size() - Idx);	      VarArgsSaveSize = XLenInBytes * (ArgRegs.size() - Idx);
      VaArgOffset = -VarArgsSaveSize;				      VaArgOffset = -VarArgsSaveSize;
    }								    }

    // Record the frame index of the first variable argument	    // Record the frame index of the first variable argument
    // which is a value necessary to VASTART.			    // which is a value necessary to VASTART.
    int FI = MFI.CreateFixedObject(XLenInBytes, VaArgOffset, 	    int FI = MFI.CreateFixedObject(XLenInBytes, VaArgOffset, 
    RVFI->setVarArgsFrameIndex(FI);				    RVFI->setVarArgsFrameIndex(FI);

    // If saving an odd number of registers then create an ex	    // If saving an odd number of registers then create an ex
    // ensure that the frame pointer is 2*XLEN-aligned, which	    // ensure that the frame pointer is 2*XLEN-aligned, which
    // offsets to even-numbered registered remain 2*XLEN-alig	    // offsets to even-numbered registered remain 2*XLEN-alig
    if (Idx % 2) {						    if (Idx % 2) {
      FI = MFI.CreateFixedObject(XLenInBytes, VaArgOffset - (	      FI = MFI.CreateFixedObject(XLenInBytes, VaArgOffset - (
                                 true);				                                 true);
      VarArgsSaveSize += XLenInBytes;				      VarArgsSaveSize += XLenInBytes;
    }								    }

    // Copy the integer registers that may have been used for	    // Copy the integer registers that may have been used for
    // to the vararg save area.					    // to the vararg save area.
    for (unsigned I = Idx; I < ArgRegs.size();			    for (unsigned I = Idx; I < ArgRegs.size();
         ++I, VaArgOffset += XLenInBytes) {			         ++I, VaArgOffset += XLenInBytes) {
      const unsigned Reg = RegInfo.createVirtualRegister(RC);	      const unsigned Reg = RegInfo.createVirtualRegister(RC);
      RegInfo.addLiveIn(ArgRegs[I], Reg);			      RegInfo.addLiveIn(ArgRegs[I], Reg);
      SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, X	      SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, X
      FI = MFI.CreateFixedObject(XLenInBytes, VaArgOffset, tr	      FI = MFI.CreateFixedObject(XLenInBytes, VaArgOffset, tr
      SDValue PtrOff = DAG.getFrameIndex(FI, getPointerTy(DAG	      SDValue PtrOff = DAG.getFrameIndex(FI, getPointerTy(DAG
      SDValue Store = DAG.getStore(Chain, DL, ArgValue, PtrOf	      SDValue Store = DAG.getStore(Chain, DL, ArgValue, PtrOf
                                   MachinePointerInfo::getFix	                                   MachinePointerInfo::getFix
      cast<StoreSDNode>(Store.getNode())			      cast<StoreSDNode>(Store.getNode())
          ->getMemOperand()					          ->getMemOperand()
          ->setValue((Value *)nullptr);				          ->setValue((Value *)nullptr);
      OutChains.push_back(Store);				      OutChains.push_back(Store);
    }								    }
    RVFI->setVarArgsSaveSize(VarArgsSaveSize);			    RVFI->setVarArgsSaveSize(VarArgsSaveSize);
  }								  }

  // All stores are grouped in one node to allow the matching	  // All stores are grouped in one node to allow the matching
  // the size of Ins and InVals. This only happens for vararg	  // the size of Ins and InVals. This only happens for vararg
  if (!OutChains.empty()) {					  if (!OutChains.empty()) {
    OutChains.push_back(Chain);					    OutChains.push_back(Chain);
    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Out	    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Out
  }								  }

  return Chain;							  return Chain;
}								}

/// IsEligibleForTailCallOptimization - Check whether the cal	/// IsEligibleForTailCallOptimization - Check whether the cal
/// for tail call optimization.					/// for tail call optimization.
/// Note: This is modelled after ARM's IsEligibleForTailCallO	/// Note: This is modelled after ARM's IsEligibleForTailCallO
bool RISCVTargetLowering::IsEligibleForTailCallOptimization(	bool RISCVTargetLowering::IsEligibleForTailCallOptimization(
  CCState &CCInfo, CallLoweringInfo &CLI, MachineFunction &MF	  CCState &CCInfo, CallLoweringInfo &CLI, MachineFunction &MF
  const SmallVector<CCValAssign, 16> &ArgLocs) const {		  const SmallVector<CCValAssign, 16> &ArgLocs) const {

  auto &Callee = CLI.Callee;					  auto &Callee = CLI.Callee;
  auto CalleeCC = CLI.CallConv;					  auto CalleeCC = CLI.CallConv;
  auto IsVarArg = CLI.IsVarArg;					  auto IsVarArg = CLI.IsVarArg;
  auto &Outs = CLI.Outs;					  auto &Outs = CLI.Outs;
  auto &Caller = MF.getFunction();				  auto &Caller = MF.getFunction();
  auto CallerCC = Caller.getCallingConv();			  auto CallerCC = Caller.getCallingConv();

  // Do not tail call opt functions with "disable-tail-calls"	  // Do not tail call opt functions with "disable-tail-calls"
  if (Caller.getFnAttribute("disable-tail-calls").getValueAsS	  if (Caller.getFnAttribute("disable-tail-calls").getValueAsS
    return false;						    return false;

  // Exception-handling functions need a special set of instr	  // Exception-handling functions need a special set of instr
  // indicate a return to the hardware. Tail-calling another 	  // indicate a return to the hardware. Tail-calling another 
  // probably break this.					  // probably break this.
  // TODO: The "interrupt" attribute isn't currently defined 	  // TODO: The "interrupt" attribute isn't currently defined 
  // should be expanded as new function attributes are introd	  // should be expanded as new function attributes are introd
  if (Caller.hasFnAttribute("interrupt"))			  if (Caller.hasFnAttribute("interrupt"))
    return false;						    return false;

  // Do not tail call opt functions with varargs.		  // Do not tail call opt functions with varargs.
  if (IsVarArg)							  if (IsVarArg)
    return false;						    return false;

  // Do not tail call opt if the stack is used to pass parame	  // Do not tail call opt if the stack is used to pass parame
  if (CCInfo.getNextStackOffset() != 0)				  if (CCInfo.getNextStackOffset() != 0)
    return false;						    return false;

  // Do not tail call opt if any parameters need to be passed	  // Do not tail call opt if any parameters need to be passed
  // Since long doubles (fp128) and i128 are larger than 2*XL	  // Since long doubles (fp128) and i128 are larger than 2*XL
  // passed indirectly. So the address of the value will be p	  // passed indirectly. So the address of the value will be p
  // register, or if not available, then the address is put o	  // register, or if not available, then the address is put o
  // order to pass indirectly, space on the stack often needs	  // order to pass indirectly, space on the stack often needs
  // in order to store the value. In this case the CCInfo.get	  // in order to store the value. In this case the CCInfo.get
  // != 0 check is not enough and we need to check if any CCV	  // != 0 check is not enough and we need to check if any CCV
  // are passed CCValAssign::Indirect.				  // are passed CCValAssign::Indirect.
  for (auto &VA : ArgLocs)					  for (auto &VA : ArgLocs)
    if (VA.getLocInfo() == CCValAssign::Indirect)		    if (VA.getLocInfo() == CCValAssign::Indirect)
      return false;						      return false;

  // Do not tail call opt if either caller or callee uses str	  // Do not tail call opt if either caller or callee uses str
  // semantics.							  // semantics.
  auto IsCallerStructRet = Caller.hasStructRetAttr();		  auto IsCallerStructRet = Caller.hasStructRetAttr();
  auto IsCalleeStructRet = Outs.empty() ? false : Outs[0].Fla	  auto IsCalleeStructRet = Outs.empty() ? false : Outs[0].Fla
  if (IsCallerStructRet || IsCalleeStructRet)			  if (IsCallerStructRet || IsCalleeStructRet)
    return false;						    return false;

  // Externally-defined functions with weak linkage should no	  // Externally-defined functions with weak linkage should no
  // tail-called. The behaviour of branch instructions in thi	  // tail-called. The behaviour of branch instructions in thi
  // used for tail calls) is implementation-defined, so we ca	  // used for tail calls) is implementation-defined, so we ca
  // linker replacing the tail call with a return.		  // linker replacing the tail call with a return.
  if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(	  if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(
    const GlobalValue *GV = G->getGlobal();			    const GlobalValue *GV = G->getGlobal();
    if (GV->hasExternalWeakLinkage())				    if (GV->hasExternalWeakLinkage())
      return false;						      return false;
  }								  }

  // The callee has to preserve all registers the caller need	  // The callee has to preserve all registers the caller need
  const RISCVRegisterInfo *TRI = Subtarget.getRegisterInfo();	  const RISCVRegisterInfo *TRI = Subtarget.getRegisterInfo();
  const uint32_t *CallerPreserved = TRI->getCallPreservedMask	  const uint32_t *CallerPreserved = TRI->getCallPreservedMask
  if (CalleeCC != CallerCC) {					  if (CalleeCC != CallerCC) {
    const uint32_t *CalleePreserved = TRI->getCallPreservedMa	    const uint32_t *CalleePreserved = TRI->getCallPreservedMa
    if (!TRI->regmaskSubsetEqual(CallerPreserved, CalleePrese	    if (!TRI->regmaskSubsetEqual(CallerPreserved, CalleePrese
      return false;						      return false;
  }								  }

  // Byval parameters hand the function a pointer directly in	  // Byval parameters hand the function a pointer directly in
  // we want to reuse during a tail call. Working around this	  // we want to reuse during a tail call. Working around this
  // but less efficient and uglier in LowerCall.		  // but less efficient and uglier in LowerCall.
  for (auto &Arg : Outs)					  for (auto &Arg : Outs)
    if (Arg.Flags.isByVal())					    if (Arg.Flags.isByVal())
      return false;						      return false;

  return true;							  return true;
}								}

// Lower a call to a callseq_start + CALL + callseq_end chain	// Lower a call to a callseq_start + CALL + callseq_end chain
// and output parameter nodes.					// and output parameter nodes.
SDValue RISCVTargetLowering::LowerCall(CallLoweringInfo &CLI,	SDValue RISCVTargetLowering::LowerCall(CallLoweringInfo &CLI,
                                       SmallVectorImpl<SDValu	                                       SmallVectorImpl<SDValu
  SelectionDAG &DAG = CLI.DAG;					  SelectionDAG &DAG = CLI.DAG;
  SDLoc &DL = CLI.DL;						  SDLoc &DL = CLI.DL;
  SmallVectorImpl<ISD::OutputArg> &Outs = CLI.Outs;		  SmallVectorImpl<ISD::OutputArg> &Outs = CLI.Outs;
  SmallVectorImpl<SDValue> &OutVals = CLI.OutVals;		  SmallVectorImpl<SDValue> &OutVals = CLI.OutVals;
  SmallVectorImpl<ISD::InputArg> &Ins = CLI.Ins;		  SmallVectorImpl<ISD::InputArg> &Ins = CLI.Ins;
  SDValue Chain = CLI.Chain;					  SDValue Chain = CLI.Chain;
  SDValue Callee = CLI.Callee;					  SDValue Callee = CLI.Callee;
  bool &IsTailCall = CLI.IsTailCall;				  bool &IsTailCall = CLI.IsTailCall;
  CallingConv::ID CallConv = CLI.CallConv;			  CallingConv::ID CallConv = CLI.CallConv;
  bool IsVarArg = CLI.IsVarArg;					  bool IsVarArg = CLI.IsVarArg;
  EVT PtrVT = getPointerTy(DAG.getDataLayout());		  EVT PtrVT = getPointerTy(DAG.getDataLayout());
  MVT XLenVT = Subtarget.getXLenVT();				  MVT XLenVT = Subtarget.getXLenVT();

  MachineFunction &MF = DAG.getMachineFunction();		  MachineFunction &MF = DAG.getMachineFunction();

  // Analyze the operands of the call, assigning locations to	  // Analyze the operands of the call, assigning locations to
  SmallVector<CCValAssign, 16> ArgLocs;				  SmallVector<CCValAssign, 16> ArgLocs;
  CCState ArgCCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.get	  CCState ArgCCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.get
  analyzeOutputArgs(MF, ArgCCInfo, Outs, /*IsRet=*/false, &CL	  analyzeOutputArgs(MF, ArgCCInfo, Outs, /*IsRet=*/false, &CL

  // Check if it's really possible to do a tail call.		  // Check if it's really possible to do a tail call.
  if (IsTailCall)						  if (IsTailCall)
    IsTailCall = IsEligibleForTailCallOptimization(ArgCCInfo,	    IsTailCall = IsEligibleForTailCallOptimization(ArgCCInfo,
                                                   ArgLocs);	                                                   ArgLocs);

  if (IsTailCall)						  if (IsTailCall)
    ++NumTailCalls;						    ++NumTailCalls;
  else if (CLI.CS && CLI.CS.isMustTailCall())			  else if (CLI.CS && CLI.CS.isMustTailCall())
    report_fatal_error("failed to perform tail call eliminati	    report_fatal_error("failed to perform tail call eliminati
                       "site marked musttail");			                       "site marked musttail");

  // Get a count of how many bytes are to be pushed on the st	  // Get a count of how many bytes are to be pushed on the st
  unsigned NumBytes = ArgCCInfo.getNextStackOffset();		  unsigned NumBytes = ArgCCInfo.getNextStackOffset();

  // Create local copies for byval args				  // Create local copies for byval args
  SmallVector<SDValue, 8> ByValArgs;				  SmallVector<SDValue, 8> ByValArgs;
  for (unsigned i = 0, e = Outs.size(); i != e; ++i) {		  for (unsigned i = 0, e = Outs.size(); i != e; ++i) {
    ISD::ArgFlagsTy Flags = Outs[i].Flags;			    ISD::ArgFlagsTy Flags = Outs[i].Flags;
    if (!Flags.isByVal())					    if (!Flags.isByVal())
      continue;							      continue;

    SDValue Arg = OutVals[i];					    SDValue Arg = OutVals[i];
    unsigned Size = Flags.getByValSize();			    unsigned Size = Flags.getByValSize();
    unsigned Align = Flags.getByValAlign();			    unsigned Align = Flags.getByValAlign();

    int FI = MF.getFrameInfo().CreateStackObject(Size, Align,	    int FI = MF.getFrameInfo().CreateStackObject(Size, Align,
    SDValue FIPtr = DAG.getFrameIndex(FI, getPointerTy(DAG.ge	    SDValue FIPtr = DAG.getFrameIndex(FI, getPointerTy(DAG.ge
    SDValue SizeNode = DAG.getConstant(Size, DL, XLenVT);	    SDValue SizeNode = DAG.getConstant(Size, DL, XLenVT);

    Chain = DAG.getMemcpy(Chain, DL, FIPtr, Arg, SizeNode, Al	    Chain = DAG.getMemcpy(Chain, DL, FIPtr, Arg, SizeNode, Al
                          /*IsVolatile=*/false,			                          /*IsVolatile=*/false,
                          /*AlwaysInline=*/false,		                          /*AlwaysInline=*/false,
                          IsTailCall, MachinePointerInfo(),	                          IsTailCall, MachinePointerInfo(),
                          MachinePointerInfo());		                          MachinePointerInfo());
    ByValArgs.push_back(FIPtr);					    ByValArgs.push_back(FIPtr);
  }								  }

  if (!IsTailCall)						  if (!IsTailCall)
    Chain = DAG.getCALLSEQ_START(Chain, NumBytes, 0, CLI.DL);	    Chain = DAG.getCALLSEQ_START(Chain, NumBytes, 0, CLI.DL);

  // Copy argument values to their designated locations.	  // Copy argument values to their designated locations.
  SmallVector<std::pair<unsigned, SDValue>, 8> RegsToPass;	  SmallVector<std::pair<unsigned, SDValue>, 8> RegsToPass;
  SmallVector<SDValue, 8> MemOpChains;				  SmallVector<SDValue, 8> MemOpChains;
  SDValue StackPtr;						  SDValue StackPtr;
  for (unsigned i = 0, j = 0, e = ArgLocs.size(); i != e; ++i	  for (unsigned i = 0, j = 0, e = ArgLocs.size(); i != e; ++i
    CCValAssign &VA = ArgLocs[i];				    CCValAssign &VA = ArgLocs[i];
    SDValue ArgValue = OutVals[i];				    SDValue ArgValue = OutVals[i];
    ISD::ArgFlagsTy Flags = Outs[i].Flags;			    ISD::ArgFlagsTy Flags = Outs[i].Flags;

    // Handle passing f64 on RV32D with a soft float ABI as a	    // Handle passing f64 on RV32D with a soft float ABI as a
    bool IsF64OnRV32DSoftABI =					    bool IsF64OnRV32DSoftABI =
        VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f6	        VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f6
    if (IsF64OnRV32DSoftABI && VA.isRegLoc()) {			    if (IsF64OnRV32DSoftABI && VA.isRegLoc()) {
      SDValue SplitF64 = DAG.getNode(				      SDValue SplitF64 = DAG.getNode(
          RISCVISD::SplitF64, DL, DAG.getVTList(MVT::i32, MVT	          RISCVISD::SplitF64, DL, DAG.getVTList(MVT::i32, MVT
      SDValue Lo = SplitF64.getValue(0);			      SDValue Lo = SplitF64.getValue(0);
      SDValue Hi = SplitF64.getValue(1);			      SDValue Hi = SplitF64.getValue(1);

      unsigned RegLo = VA.getLocReg();				      unsigned RegLo = VA.getLocReg();
      RegsToPass.push_back(std::make_pair(RegLo, Lo));		      RegsToPass.push_back(std::make_pair(RegLo, Lo));

      if (RegLo == RISCV::X17) {				      if (RegLo == RISCV::X17) {
        // Second half of f64 is passed on the stack.		        // Second half of f64 is passed on the stack.
        // Work out the address of the stack slot.		        // Work out the address of the stack slot.
        if (!StackPtr.getNode())				        if (!StackPtr.getNode())
          StackPtr = DAG.getCopyFromReg(Chain, DL, RISCV::X2,	          StackPtr = DAG.getCopyFromReg(Chain, DL, RISCV::X2,
        // Emit the store.					        // Emit the store.
        MemOpChains.push_back(					        MemOpChains.push_back(
            DAG.getStore(Chain, DL, Hi, StackPtr, MachinePoin	            DAG.getStore(Chain, DL, Hi, StackPtr, MachinePoin
      } else {							      } else {
        // Second half of f64 is passed in another GPR.		        // Second half of f64 is passed in another GPR.
        unsigned RegHigh = RegLo + 1;				        unsigned RegHigh = RegLo + 1;
        RegsToPass.push_back(std::make_pair(RegHigh, Hi));	        RegsToPass.push_back(std::make_pair(RegHigh, Hi));
      }								      }
      continue;							      continue;
    }								    }

    // IsF64OnRV32DSoftABI && VA.isMemLoc() is handled below 	    // IsF64OnRV32DSoftABI && VA.isMemLoc() is handled below 
    // as any other MemLoc.					    // as any other MemLoc.

    // Promote the value if needed.				    // Promote the value if needed.
    // For now, only handle fully promoted and indirect argum	    // For now, only handle fully promoted and indirect argum
    if (VA.getLocInfo() == CCValAssign::Indirect) {		    if (VA.getLocInfo() == CCValAssign::Indirect) {
      // Store the argument in a stack slot and pass its addr	      // Store the argument in a stack slot and pass its addr
      SDValue SpillSlot = DAG.CreateStackTemporary(Outs[i].Ar	      SDValue SpillSlot = DAG.CreateStackTemporary(Outs[i].Ar
      int FI = cast<FrameIndexSDNode>(SpillSlot)->getIndex();	      int FI = cast<FrameIndexSDNode>(SpillSlot)->getIndex();
      MemOpChains.push_back(					      MemOpChains.push_back(
          DAG.getStore(Chain, DL, ArgValue, SpillSlot,		          DAG.getStore(Chain, DL, ArgValue, SpillSlot,
                       MachinePointerInfo::getFixedStack(MF, 	                       MachinePointerInfo::getFixedStack(MF, 
      // If the original argument was split (e.g. i128), we n	      // If the original argument was split (e.g. i128), we n
      // to store all parts of it here (and pass just one add	      // to store all parts of it here (and pass just one add
      unsigned ArgIndex = Outs[i].OrigArgIndex;			      unsigned ArgIndex = Outs[i].OrigArgIndex;
      assert(Outs[i].PartOffset == 0);				      assert(Outs[i].PartOffset == 0);
      while (i + 1 != e && Outs[i + 1].OrigArgIndex == ArgInd	      while (i + 1 != e && Outs[i + 1].OrigArgIndex == ArgInd
        SDValue PartValue = OutVals[i + 1];			        SDValue PartValue = OutVals[i + 1];
        unsigned PartOffset = Outs[i + 1].PartOffset;		        unsigned PartOffset = Outs[i + 1].PartOffset;
        SDValue Address = DAG.getNode(ISD::ADD, DL, PtrVT, Sp	        SDValue Address = DAG.getNode(ISD::ADD, DL, PtrVT, Sp
                                      DAG.getIntPtrConstant(P	                                      DAG.getIntPtrConstant(P
        MemOpChains.push_back(					        MemOpChains.push_back(
            DAG.getStore(Chain, DL, PartValue, Address,		            DAG.getStore(Chain, DL, PartValue, Address,
                         MachinePointerInfo::getFixedStack(MF	                         MachinePointerInfo::getFixedStack(MF
        ++i;							        ++i;
      }								      }
      ArgValue = SpillSlot;					      ArgValue = SpillSlot;
    } else {							    } else {
      ArgValue = convertValVTToLocVT(DAG, ArgValue, VA, DL);	      ArgValue = convertValVTToLocVT(DAG, ArgValue, VA, DL);
    }								    }

    // Use local copy if it is a byval arg.			    // Use local copy if it is a byval arg.
    if (Flags.isByVal())					    if (Flags.isByVal())
      ArgValue = ByValArgs[j++];				      ArgValue = ByValArgs[j++];

    if (VA.isRegLoc()) {					    if (VA.isRegLoc()) {
      // Queue up the argument copies and emit them at the en	      // Queue up the argument copies and emit them at the en
      RegsToPass.push_back(std::make_pair(VA.getLocReg(), Arg	      RegsToPass.push_back(std::make_pair(VA.getLocReg(), Arg
    } else {							    } else {
      assert(VA.isMemLoc() && "Argument not register or memor	      assert(VA.isMemLoc() && "Argument not register or memor
      assert(!IsTailCall && "Tail call not allowed if stack i	      assert(!IsTailCall && "Tail call not allowed if stack i
                            "for passing parameters");		                            "for passing parameters");

      // Work out the address of the stack slot.		      // Work out the address of the stack slot.
      if (!StackPtr.getNode())					      if (!StackPtr.getNode())
        StackPtr = DAG.getCopyFromReg(Chain, DL, RISCV::X2, P	        StackPtr = DAG.getCopyFromReg(Chain, DL, RISCV::X2, P
      SDValue Address =						      SDValue Address =
          DAG.getNode(ISD::ADD, DL, PtrVT, StackPtr,		          DAG.getNode(ISD::ADD, DL, PtrVT, StackPtr,
                      DAG.getIntPtrConstant(VA.getLocMemOffse	                      DAG.getIntPtrConstant(VA.getLocMemOffse

      // Emit the store.					      // Emit the store.
      MemOpChains.push_back(					      MemOpChains.push_back(
          DAG.getStore(Chain, DL, ArgValue, Address, MachineP	          DAG.getStore(Chain, DL, ArgValue, Address, MachineP
    }								    }
  }								  }

  // Join the stores, which are independent of one another.	  // Join the stores, which are independent of one another.
  if (!MemOpChains.empty())					  if (!MemOpChains.empty())
    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Mem	    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, Mem

  SDValue Glue;							  SDValue Glue;

  // Build a sequence of copy-to-reg nodes, chained and glued	  // Build a sequence of copy-to-reg nodes, chained and glued
  for (auto &Reg : RegsToPass) {				  for (auto &Reg : RegsToPass) {
    Chain = DAG.getCopyToReg(Chain, DL, Reg.first, Reg.second	    Chain = DAG.getCopyToReg(Chain, DL, Reg.first, Reg.second
    Glue = Chain.getValue(1);					    Glue = Chain.getValue(1);
  }								  }

  // If the callee is a GlobalAddress/ExternalSymbol node, tu	  // If the callee is a GlobalAddress/ExternalSymbol node, tu
  // TargetGlobalAddress/TargetExternalSymbol node so that le	  // TargetGlobalAddress/TargetExternalSymbol node so that le
  // split it and then direct call can be matched by PseudoCA	  // split it and then direct call can be matched by PseudoCA
  if (GlobalAddressSDNode *S = dyn_cast<GlobalAddressSDNode>(	  if (GlobalAddressSDNode *S = dyn_cast<GlobalAddressSDNode>(
    Callee = DAG.getTargetGlobalAddress(S->getGlobal(), DL, P	    Callee = DAG.getTargetGlobalAddress(S->getGlobal(), DL, P
  } else if (ExternalSymbolSDNode *S = dyn_cast<ExternalSymbo	  } else if (ExternalSymbolSDNode *S = dyn_cast<ExternalSymbo
    Callee = DAG.getTargetExternalSymbol(S->getSymbol(), PtrV	    Callee = DAG.getTargetExternalSymbol(S->getSymbol(), PtrV
  }								  }

  // The first call operand is the chain and the second is th	  // The first call operand is the chain and the second is th
  SmallVector<SDValue, 8> Ops;					  SmallVector<SDValue, 8> Ops;
  Ops.push_back(Chain);						  Ops.push_back(Chain);
  Ops.push_back(Callee);					  Ops.push_back(Callee);

  // Add argument registers to the end of the list so that th	  // Add argument registers to the end of the list so that th
  // known live into the call.					  // known live into the call.
  for (auto &Reg : RegsToPass)					  for (auto &Reg : RegsToPass)
    Ops.push_back(DAG.getRegister(Reg.first, Reg.second.getVa	    Ops.push_back(DAG.getRegister(Reg.first, Reg.second.getVa

  if (!IsTailCall) {						  if (!IsTailCall) {
    // Add a register mask operand representing the call-pres	    // Add a register mask operand representing the call-pres
    const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo	    const TargetRegisterInfo *TRI = Subtarget.getRegisterInfo
    const uint32_t *Mask = TRI->getCallPreservedMask(MF, Call	    const uint32_t *Mask = TRI->getCallPreservedMask(MF, Call
    assert(Mask && "Missing call preserved mask for calling c	    assert(Mask && "Missing call preserved mask for calling c
    Ops.push_back(DAG.getRegisterMask(Mask));			    Ops.push_back(DAG.getRegisterMask(Mask));
  }								  }

  // Glue the call to the argument copies, if any.		  // Glue the call to the argument copies, if any.
  if (Glue.getNode())						  if (Glue.getNode())
    Ops.push_back(Glue);					    Ops.push_back(Glue);

  // Emit the call.						  // Emit the call.
  SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);	  SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);

  if (IsTailCall) {						  if (IsTailCall) {
    MF.getFrameInfo().setHasTailCall();				    MF.getFrameInfo().setHasTailCall();
    return DAG.getNode(RISCVISD::TAIL, DL, NodeTys, Ops);	    return DAG.getNode(RISCVISD::TAIL, DL, NodeTys, Ops);
  }								  }

  Chain = DAG.getNode(RISCVISD::CALL, DL, NodeTys, Ops);	  Chain = DAG.getNode(RISCVISD::CALL, DL, NodeTys, Ops);
  Glue = Chain.getValue(1);					  Glue = Chain.getValue(1);

  // Mark the end of the call, which is glued to the call its	  // Mark the end of the call, which is glued to the call its
  Chain = DAG.getCALLSEQ_END(Chain,				  Chain = DAG.getCALLSEQ_END(Chain,
                             DAG.getConstant(NumBytes, DL, Pt	                             DAG.getConstant(NumBytes, DL, Pt
                             DAG.getConstant(0, DL, PtrVT, tr	                             DAG.getConstant(0, DL, PtrVT, tr
                             Glue, DL);				                             Glue, DL);
  Glue = Chain.getValue(1);					  Glue = Chain.getValue(1);

  // Assign locations to each value returned by this call.	  // Assign locations to each value returned by this call.
  SmallVector<CCValAssign, 16> RVLocs;				  SmallVector<CCValAssign, 16> RVLocs;
  CCState RetCCInfo(CallConv, IsVarArg, MF, RVLocs, *DAG.getC	  CCState RetCCInfo(CallConv, IsVarArg, MF, RVLocs, *DAG.getC
  analyzeInputArgs(MF, RetCCInfo, Ins, /*IsRet=*/true);		  analyzeInputArgs(MF, RetCCInfo, Ins, /*IsRet=*/true);

  // Copy all of the result registers out of their specified 	  // Copy all of the result registers out of their specified 
  for (auto &VA : RVLocs) {					  for (auto &VA : RVLocs) {
    // Copy the value out					    // Copy the value out
    SDValue RetValue =						    SDValue RetValue =
        DAG.getCopyFromReg(Chain, DL, VA.getLocReg(), VA.getL	        DAG.getCopyFromReg(Chain, DL, VA.getLocReg(), VA.getL
    // Glue the RetValue to the end of the call sequence	    // Glue the RetValue to the end of the call sequence
    Chain = RetValue.getValue(1);				    Chain = RetValue.getValue(1);
    Glue = RetValue.getValue(2);				    Glue = RetValue.getValue(2);

    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f6	    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f6
      assert(VA.getLocReg() == ArgGPRs[0] && "Unexpected reg 	      assert(VA.getLocReg() == ArgGPRs[0] && "Unexpected reg 
      SDValue RetValue2 =					      SDValue RetValue2 =
          DAG.getCopyFromReg(Chain, DL, ArgGPRs[1], MVT::i32,	          DAG.getCopyFromReg(Chain, DL, ArgGPRs[1], MVT::i32,
      Chain = RetValue2.getValue(1);				      Chain = RetValue2.getValue(1);
      Glue = RetValue2.getValue(2);				      Glue = RetValue2.getValue(2);
      RetValue = DAG.getNode(RISCVISD::BuildPairF64, DL, MVT:	      RetValue = DAG.getNode(RISCVISD::BuildPairF64, DL, MVT:
                             RetValue2);			                             RetValue2);
    }								    }

    RetValue = convertLocVTToValVT(DAG, RetValue, VA, DL);	    RetValue = convertLocVTToValVT(DAG, RetValue, VA, DL);

    InVals.push_back(RetValue);					    InVals.push_back(RetValue);
  }								  }

  return Chain;							  return Chain;
}								}

bool RISCVTargetLowering::CanLowerReturn(			bool RISCVTargetLowering::CanLowerReturn(
    CallingConv::ID CallConv, MachineFunction &MF, bool IsVar	    CallingConv::ID CallConv, MachineFunction &MF, bool IsVar
    const SmallVectorImpl<ISD::OutputArg> &Outs, LLVMContext 	    const SmallVectorImpl<ISD::OutputArg> &Outs, LLVMContext 
  SmallVector<CCValAssign, 16> RVLocs;				  SmallVector<CCValAssign, 16> RVLocs;
  CCState CCInfo(CallConv, IsVarArg, MF, RVLocs, Context);	  CCState CCInfo(CallConv, IsVarArg, MF, RVLocs, Context);
  for (unsigned i = 0, e = Outs.size(); i != e; ++i) {		  for (unsigned i = 0, e = Outs.size(); i != e; ++i) {
    MVT VT = Outs[i].VT;					    MVT VT = Outs[i].VT;
    ISD::ArgFlagsTy ArgFlags = Outs[i].Flags;			    ISD::ArgFlagsTy ArgFlags = Outs[i].Flags;
    if (CC_RISCV(MF.getDataLayout(), i, VT, VT, CCValAssign::	    if (CC_RISCV(MF.getDataLayout(), i, VT, VT, CCValAssign::
                 CCInfo, /*IsFixed=*/true, /*IsRet=*/true, nu	                 CCInfo, /*IsFixed=*/true, /*IsRet=*/true, nu
      return false;						      return false;
  }								  }
  return true;							  return true;
}								}

SDValue								SDValue
RISCVTargetLowering::LowerReturn(SDValue Chain, CallingConv::	RISCVTargetLowering::LowerReturn(SDValue Chain, CallingConv::
                                 bool IsVarArg,			                                 bool IsVarArg,
                                 const SmallVectorImpl<ISD::O	                                 const SmallVectorImpl<ISD::O
                                 const SmallVectorImpl<SDValu	                                 const SmallVectorImpl<SDValu
                                 const SDLoc &DL, SelectionDA	                                 const SDLoc &DL, SelectionDA
  // Stores the assignment of the return value to a location.	  // Stores the assignment of the return value to a location.
  SmallVector<CCValAssign, 16> RVLocs;				  SmallVector<CCValAssign, 16> RVLocs;

  // Info about the registers and stack slot.			  // Info about the registers and stack slot.
  CCState CCInfo(CallConv, IsVarArg, DAG.getMachineFunction()	  CCState CCInfo(CallConv, IsVarArg, DAG.getMachineFunction()
                 *DAG.getContext());				                 *DAG.getContext());

  analyzeOutputArgs(DAG.getMachineFunction(), CCInfo, Outs, /	  analyzeOutputArgs(DAG.getMachineFunction(), CCInfo, Outs, /
                    nullptr);					                    nullptr);

  SDValue Glue;							  SDValue Glue;
  SmallVector<SDValue, 4> RetOps(1, Chain);			  SmallVector<SDValue, 4> RetOps(1, Chain);

  // Copy the result values into the output registers.		  // Copy the result values into the output registers.
  for (unsigned i = 0, e = RVLocs.size(); i < e; ++i) {		  for (unsigned i = 0, e = RVLocs.size(); i < e; ++i) {
    SDValue Val = OutVals[i];					    SDValue Val = OutVals[i];
    CCValAssign &VA = RVLocs[i];				    CCValAssign &VA = RVLocs[i];
    assert(VA.isRegLoc() && "Can only return in registers!");	    assert(VA.isRegLoc() && "Can only return in registers!");

    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f6	    if (VA.getLocVT() == MVT::i32 && VA.getValVT() == MVT::f6
      // Handle returning f64 on RV32D with a soft float ABI.	      // Handle returning f64 on RV32D with a soft float ABI.
      assert(VA.isRegLoc() && "Expected return via registers"	      assert(VA.isRegLoc() && "Expected return via registers"
      SDValue SplitF64 = DAG.getNode(RISCVISD::SplitF64, DL,	      SDValue SplitF64 = DAG.getNode(RISCVISD::SplitF64, DL,
                                     DAG.getVTList(MVT::i32, 	                                     DAG.getVTList(MVT::i32, 
      SDValue Lo = SplitF64.getValue(0);			      SDValue Lo = SplitF64.getValue(0);
      SDValue Hi = SplitF64.getValue(1);			      SDValue Hi = SplitF64.getValue(1);
      unsigned RegLo = VA.getLocReg();				      unsigned RegLo = VA.getLocReg();
      unsigned RegHi = RegLo + 1;				      unsigned RegHi = RegLo + 1;
      Chain = DAG.getCopyToReg(Chain, DL, RegLo, Lo, Glue);	      Chain = DAG.getCopyToReg(Chain, DL, RegLo, Lo, Glue);
      Glue = Chain.getValue(1);					      Glue = Chain.getValue(1);
      RetOps.push_back(DAG.getRegister(RegLo, MVT::i32));	      RetOps.push_back(DAG.getRegister(RegLo, MVT::i32));
      Chain = DAG.getCopyToReg(Chain, DL, RegHi, Hi, Glue);	      Chain = DAG.getCopyToReg(Chain, DL, RegHi, Hi, Glue);
      Glue = Chain.getValue(1);					      Glue = Chain.getValue(1);
      RetOps.push_back(DAG.getRegister(RegHi, MVT::i32));	      RetOps.push_back(DAG.getRegister(RegHi, MVT::i32));
    } else {							    } else {
      // Handle a 'normal' return.				      // Handle a 'normal' return.
      Val = convertValVTToLocVT(DAG, Val, VA, DL);		      Val = convertValVTToLocVT(DAG, Val, VA, DL);
      Chain = DAG.getCopyToReg(Chain, DL, VA.getLocReg(), Val	      Chain = DAG.getCopyToReg(Chain, DL, VA.getLocReg(), Val

      // Guarantee that all emitted copies are stuck together	      // Guarantee that all emitted copies are stuck together
      Glue = Chain.getValue(1);					      Glue = Chain.getValue(1);
      RetOps.push_back(DAG.getRegister(VA.getLocReg(), VA.get	      RetOps.push_back(DAG.getRegister(VA.getLocReg(), VA.get
    }								    }
  }								  }

  RetOps[0] = Chain; // Update chain.				  RetOps[0] = Chain; // Update chain.

  // Add the glue node if we have it.				  // Add the glue node if we have it.
  if (Glue.getNode()) {						  if (Glue.getNode()) {
    RetOps.push_back(Glue);					    RetOps.push_back(Glue);
  }								  }

  // Interrupt service routines use different return instruct	  // Interrupt service routines use different return instruct
  const Function &Func = DAG.getMachineFunction().getFunction	  const Function &Func = DAG.getMachineFunction().getFunction
  if (Func.hasFnAttribute("interrupt")) {			  if (Func.hasFnAttribute("interrupt")) {
    if (!Func.getReturnType()->isVoidTy())			    if (!Func.getReturnType()->isVoidTy())
      report_fatal_error(					      report_fatal_error(
          "Functions with the interrupt attribute must have v	          "Functions with the interrupt attribute must have v

    MachineFunction &MF = DAG.getMachineFunction();		    MachineFunction &MF = DAG.getMachineFunction();
    StringRef Kind =						    StringRef Kind =
      MF.getFunction().getFnAttribute("interrupt").getValueAs	      MF.getFunction().getFnAttribute("interrupt").getValueAs

    unsigned RetOpc;						    unsigned RetOpc;
    if (Kind == "user")						    if (Kind == "user")
      RetOpc = RISCVISD::URET_FLAG;				      RetOpc = RISCVISD::URET_FLAG;
    else if (Kind == "supervisor")				    else if (Kind == "supervisor")
      RetOpc = RISCVISD::SRET_FLAG;				      RetOpc = RISCVISD::SRET_FLAG;
    else							    else
      RetOpc = RISCVISD::MRET_FLAG;				      RetOpc = RISCVISD::MRET_FLAG;

    return DAG.getNode(RetOpc, DL, MVT::Other, RetOps);		    return DAG.getNode(RetOpc, DL, MVT::Other, RetOps);
  }								  }

  return DAG.getNode(RISCVISD::RET_FLAG, DL, MVT::Other, RetO	  return DAG.getNode(RISCVISD::RET_FLAG, DL, MVT::Other, RetO
}								}

const char *RISCVTargetLowering::getTargetNodeName(unsigned O	const char *RISCVTargetLowering::getTargetNodeName(unsigned O
  switch ((RISCVISD::NodeType)Opcode) {				  switch ((RISCVISD::NodeType)Opcode) {
  case RISCVISD::FIRST_NUMBER:					  case RISCVISD::FIRST_NUMBER:
    break;							    break;
  case RISCVISD::RET_FLAG:					  case RISCVISD::RET_FLAG:
    return "RISCVISD::RET_FLAG";				    return "RISCVISD::RET_FLAG";
  case RISCVISD::URET_FLAG:					  case RISCVISD::URET_FLAG:
    return "RISCVISD::URET_FLAG";				    return "RISCVISD::URET_FLAG";
  case RISCVISD::SRET_FLAG:					  case RISCVISD::SRET_FLAG:
    return "RISCVISD::SRET_FLAG";				    return "RISCVISD::SRET_FLAG";
  case RISCVISD::MRET_FLAG:					  case RISCVISD::MRET_FLAG:
    return "RISCVISD::MRET_FLAG";				    return "RISCVISD::MRET_FLAG";
  case RISCVISD::CALL:						  case RISCVISD::CALL:
    return "RISCVISD::CALL";					    return "RISCVISD::CALL";
  case RISCVISD::SELECT_CC:					  case RISCVISD::SELECT_CC:
    return "RISCVISD::SELECT_CC";				    return "RISCVISD::SELECT_CC";
  case RISCVISD::BuildPairF64:					  case RISCVISD::BuildPairF64:
    return "RISCVISD::BuildPairF64";				    return "RISCVISD::BuildPairF64";
  case RISCVISD::SplitF64:					  case RISCVISD::SplitF64:
    return "RISCVISD::SplitF64";				    return "RISCVISD::SplitF64";
  case RISCVISD::TAIL:						  case RISCVISD::TAIL:
    return "RISCVISD::TAIL";					    return "RISCVISD::TAIL";
  }								  }
  return nullptr;						  return nullptr;
}								}

std::pair<unsigned, const TargetRegisterClass *>		std::pair<unsigned, const TargetRegisterClass *>
RISCVTargetLowering::getRegForInlineAsmConstraint(const Targe	RISCVTargetLowering::getRegForInlineAsmConstraint(const Targe
                                                  StringRef C	                                                  StringRef C
                                                  MVT VT) con	                                                  MVT VT) con
  // First, see if this is a constraint that directly corresp	  // First, see if this is a constraint that directly corresp
  // RISCV register class.					  // RISCV register class.
  if (Constraint.size() == 1) {					  if (Constraint.size() == 1) {
    switch (Constraint[0]) {					    switch (Constraint[0]) {
    case 'r':							    case 'r':
      return std::make_pair(0U, &RISCV::GPRRegClass);		      return std::make_pair(0U, &RISCV::GPRRegClass);
    default:							    default:
      break;							      break;
    }								    }
  }								  }

  return TargetLowering::getRegForInlineAsmConstraint(TRI, Co	  return TargetLowering::getRegForInlineAsmConstraint(TRI, Co
}								}

Instruction *RISCVTargetLowering::emitLeadingFence(IRBuilder<	Instruction *RISCVTargetLowering::emitLeadingFence(IRBuilder<
                                                   Instructio	                                                   Instructio
                                                   AtomicOrde	                                                   AtomicOrde
  if (isa<LoadInst>(Inst) && Ord == AtomicOrdering::Sequentia	  if (isa<LoadInst>(Inst) && Ord == AtomicOrdering::Sequentia
    return Builder.CreateFence(Ord);				    return Builder.CreateFence(Ord);
  if (isa<StoreInst>(Inst) && isReleaseOrStronger(Ord))		  if (isa<StoreInst>(Inst) && isReleaseOrStronger(Ord))
    return Builder.CreateFence(AtomicOrdering::Release);	    return Builder.CreateFence(AtomicOrdering::Release);
  return nullptr;						  return nullptr;
}								}

Instruction *RISCVTargetLowering::emitTrailingFence(IRBuilder	Instruction *RISCVTargetLowering::emitTrailingFence(IRBuilder
                                                    Instructi	                                                    Instructi
                                                    AtomicOrd	                                                    AtomicOrd
  if (isa<LoadInst>(Inst) && isAcquireOrStronger(Ord))		  if (isa<LoadInst>(Inst) && isAcquireOrStronger(Ord))
    return Builder.CreateFence(AtomicOrdering::Acquire);	    return Builder.CreateFence(AtomicOrdering::Acquire);
  return nullptr;						  return nullptr;
}								}

TargetLowering::AtomicExpansionKind				TargetLowering::AtomicExpansionKind
RISCVTargetLowering::shouldExpandAtomicRMWInIR(AtomicRMWInst 	RISCVTargetLowering::shouldExpandAtomicRMWInIR(AtomicRMWInst 
  unsigned Size = AI->getType()->getPrimitiveSizeInBits();	  unsigned Size = AI->getType()->getPrimitiveSizeInBits();
  if (Size == 8 || Size == 16)					  if (Size == 8 || Size == 16)
    return AtomicExpansionKind::MaskedIntrinsic;		    return AtomicExpansionKind::MaskedIntrinsic;
  return AtomicExpansionKind::None;				  return AtomicExpansionKind::None;
}								}

static Intrinsic::ID						static Intrinsic::ID
getIntrinsicForMaskedAtomicRMWBinOp(unsigned XLen, AtomicRMWI |	getIntrinsicForMaskedAtomicRMWBinOp32(AtomicRMWInst::BinOp Bi
  if (XLen == 32) {					      |	  switch (BinOp) {
    switch (BinOp) {					      |	  default:
    default:						      |	    llvm_unreachable("Unexpected AtomicRMW BinOp");
      llvm_unreachable("Unexpected AtomicRMW BinOp");	      |	  case AtomicRMWInst::Xchg:
    case AtomicRMWInst::Xchg:				      |	    return Intrinsic::riscv_masked_atomicrmw_xchg_i32;
      return Intrinsic::riscv_masked_atomicrmw_xchg_i32;      |	  case AtomicRMWInst::Add:
    case AtomicRMWInst::Add:				      |	    return Intrinsic::riscv_masked_atomicrmw_add_i32;
      return Intrinsic::riscv_masked_atomicrmw_add_i32;	      |	  case AtomicRMWInst::Sub:
    case AtomicRMWInst::Sub:				      |	    return Intrinsic::riscv_masked_atomicrmw_sub_i32;
      return Intrinsic::riscv_masked_atomicrmw_sub_i32;	      |	  case AtomicRMWInst::Nand:
    case AtomicRMWInst::Nand:				      |	    return Intrinsic::riscv_masked_atomicrmw_nand_i32;
      return Intrinsic::riscv_masked_atomicrmw_nand_i32;      |	  case AtomicRMWInst::Max:
    case AtomicRMWInst::Max:				      |	    return Intrinsic::riscv_masked_atomicrmw_max_i32;
      return Intrinsic::riscv_masked_atomicrmw_max_i32;	      |	  case AtomicRMWInst::Min:
    case AtomicRMWInst::Min:				      |	    return Intrinsic::riscv_masked_atomicrmw_min_i32;
      return Intrinsic::riscv_masked_atomicrmw_min_i32;	      |	  case AtomicRMWInst::UMax:
    case AtomicRMWInst::UMax:				      |	    return Intrinsic::riscv_masked_atomicrmw_umax_i32;
      return Intrinsic::riscv_masked_atomicrmw_umax_i32;      |	  case AtomicRMWInst::UMin:
    case AtomicRMWInst::UMin:				      |	    return Intrinsic::riscv_masked_atomicrmw_umin_i32;
      return Intrinsic::riscv_masked_atomicrmw_umin_i32;      <
    }							      <
  }							      <
							      <
  if (XLen == 64) {					      <
    switch (BinOp) {					      <
    default:						      <
      llvm_unreachable("Unexpected AtomicRMW BinOp");	      <
    case AtomicRMWInst::Xchg:				      <
      return Intrinsic::riscv_masked_atomicrmw_xchg_i64;      <
    case AtomicRMWInst::Add:				      <
      return Intrinsic::riscv_masked_atomicrmw_add_i64;	      <
    case AtomicRMWInst::Sub:				      <
      return Intrinsic::riscv_masked_atomicrmw_sub_i64;	      <
    case AtomicRMWInst::Nand:				      <
      return Intrinsic::riscv_masked_atomicrmw_nand_i64;      <
    case AtomicRMWInst::Max:				      <
      return Intrinsic::riscv_masked_atomicrmw_max_i64;	      <
    case AtomicRMWInst::Min:				      <
      return Intrinsic::riscv_masked_atomicrmw_min_i64;	      <
    case AtomicRMWInst::UMax:				      <
      return Intrinsic::riscv_masked_atomicrmw_umax_i64;      <
    case AtomicRMWInst::UMin:				      <
      return Intrinsic::riscv_masked_atomicrmw_umin_i64;      <
    }							      <
  }								  }
							      <
  llvm_unreachable("Unexpected XLen\n");		      <
}								}

Value *RISCVTargetLowering::emitMaskedAtomicRMWIntrinsic(	Value *RISCVTargetLowering::emitMaskedAtomicRMWIntrinsic(
    IRBuilder<> &Builder, AtomicRMWInst *AI, Value *AlignedAd	    IRBuilder<> &Builder, AtomicRMWInst *AI, Value *AlignedAd
    Value *Mask, Value *ShiftAmt, AtomicOrdering Ord) const {	    Value *Mask, Value *ShiftAmt, AtomicOrdering Ord) const {
  unsigned XLen = Subtarget.getXLen();			      |	  Value *Ordering = Builder.getInt32(static_cast<uint32_t>(AI
  Value *Ordering =					      <
      Builder.getIntN(XLen, static_cast<uint64_t>(AI->getOrde <
  Type *Tys[] = {AlignedAddr->getType()};			  Type *Tys[] = {AlignedAddr->getType()};
  Function *LrwOpScwLoop = Intrinsic::getDeclaration(		  Function *LrwOpScwLoop = Intrinsic::getDeclaration(
      AI->getModule(),						      AI->getModule(),
      getIntrinsicForMaskedAtomicRMWBinOp(XLen, AI->getOperat |	      getIntrinsicForMaskedAtomicRMWBinOp32(AI->getOperation(
							      <
  if (XLen == 64) {					      <
    Incr = Builder.CreateSExt(Incr, Builder.getInt64Ty());    <
    Mask = Builder.CreateSExt(Mask, Builder.getInt64Ty());    <
    ShiftAmt = Builder.CreateSExt(ShiftAmt, Builder.getInt64T <
  }							      <
							      <
  Value *Result;					      <

  // Must pass the shift amount needed to sign extend the loa	  // Must pass the shift amount needed to sign extend the loa
  // to performing a signed comparison for min/max. ShiftAmt 	  // to performing a signed comparison for min/max. ShiftAmt 
  // bits to shift the value into position. Pass XLen-ShiftAm	  // bits to shift the value into position. Pass XLen-ShiftAm
  // is the number of bits to left+right shift the value in o	  // is the number of bits to left+right shift the value in o
  // sign-extend.						  // sign-extend.
  if (AI->getOperation() == AtomicRMWInst::Min ||		  if (AI->getOperation() == AtomicRMWInst::Min ||
      AI->getOperation() == AtomicRMWInst::Max) {		      AI->getOperation() == AtomicRMWInst::Max) {
    const DataLayout &DL = AI->getModule()->getDataLayout();	    const DataLayout &DL = AI->getModule()->getDataLayout();
    unsigned ValWidth =						    unsigned ValWidth =
        DL.getTypeStoreSizeInBits(AI->getValOperand()->getTyp	        DL.getTypeStoreSizeInBits(AI->getValOperand()->getTyp
    Value *SextShamt =					      |	    Value *SextShamt = Builder.CreateSub(
        Builder.CreateSub(Builder.getIntN(XLen, XLen - ValWid |	        Builder.getInt32(Subtarget.getXLen() - ValWidth), Shi
    Result = Builder.CreateCall(LrwOpScwLoop,		      |	    return Builder.CreateCall(LrwOpScwLoop,
                                {AlignedAddr, Incr, Mask, Sex |	                              {AlignedAddr, Incr, Mask, SextS
  } else {						      <
    Result =						      <
        Builder.CreateCall(LrwOpScwLoop, {AlignedAddr, Incr,  <
  }								  }

  if (XLen == 64)					      |	  return Builder.CreateCall(LrwOpScwLoop, {AlignedAddr, Incr,
    Result = Builder.CreateTrunc(Result, Builder.getInt32Ty() <
  return Result;					      <
}								}

TargetLowering::AtomicExpansionKind				TargetLowering::AtomicExpansionKind
RISCVTargetLowering::shouldExpandAtomicCmpXchgInIR(		RISCVTargetLowering::shouldExpandAtomicCmpXchgInIR(
    AtomicCmpXchgInst *CI) const {				    AtomicCmpXchgInst *CI) const {
  unsigned Size = CI->getCompareOperand()->getType()->getPrim	  unsigned Size = CI->getCompareOperand()->getType()->getPrim
  if (Size == 8 || Size == 16)					  if (Size == 8 || Size == 16)
    return AtomicExpansionKind::MaskedIntrinsic;		    return AtomicExpansionKind::MaskedIntrinsic;
  return AtomicExpansionKind::None;				  return AtomicExpansionKind::None;
}								}

Value *RISCVTargetLowering::emitMaskedAtomicCmpXchgIntrinsic(	Value *RISCVTargetLowering::emitMaskedAtomicCmpXchgIntrinsic(
    IRBuilder<> &Builder, AtomicCmpXchgInst *CI, Value *Align	    IRBuilder<> &Builder, AtomicCmpXchgInst *CI, Value *Align
    Value *CmpVal, Value *NewVal, Value *Mask, AtomicOrdering	    Value *CmpVal, Value *NewVal, Value *Mask, AtomicOrdering
  unsigned XLen = Subtarget.getXLen();			      |	  Value *Ordering = Builder.getInt32(static_cast<uint32_t>(Or
  Value *Ordering = Builder.getIntN(XLen, static_cast<uint64_ <
  Intrinsic::ID CmpXchgIntrID = Intrinsic::riscv_masked_cmpxc <
  if (XLen == 64) {					      <
    CmpVal = Builder.CreateSExt(CmpVal, Builder.getInt64Ty()) <
    NewVal = Builder.CreateSExt(NewVal, Builder.getInt64Ty()) <
    Mask = Builder.CreateSExt(Mask, Builder.getInt64Ty());    <
    CmpXchgIntrID = Intrinsic::riscv_masked_cmpxchg_i64;      <
  }							      <
  Type *Tys[] = {AlignedAddr->getType()};			  Type *Tys[] = {AlignedAddr->getType()};
  Function *MaskedCmpXchg =				      |	  Function *MaskedCmpXchg = Intrinsic::getDeclaration(
      Intrinsic::getDeclaration(CI->getModule(), CmpXchgIntrI |	      CI->getModule(), Intrinsic::riscv_masked_cmpxchg_i32, T
  Value *Result = Builder.CreateCall(			      |	  return Builder.CreateCall(MaskedCmpXchg,
      MaskedCmpXchg, {AlignedAddr, CmpVal, NewVal, Mask, Orde |	                            {AlignedAddr, CmpVal, NewVal, Mas
  if (XLen == 64)					      <
    Result = Builder.CreateTrunc(Result, Builder.getInt32Ty() <
  return Result;					      <
}								}
